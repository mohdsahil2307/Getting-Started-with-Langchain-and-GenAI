{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.text.TextLoader at 0x14574b85930>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = TextLoader('speech.txt')\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'speech.txt'}, page_content='...In connection with the Jewish question I have this to say: it is a shameful spectacle to see how the whole democratic world is oozing sympathy for the poor tormented Jewish people, but remains hard-hearted and obdurate when it comes to helping them which is surely, in view of its attitude, an obvious duty. The arguments that are brought up as an excuse for not helping them actually speak for us Germans and Italians.\\n\\nFor this is what they say:\\n\\n1. \"We,\" that is the democracies, \"are not in a position to take in the Jews.\" Yet in these empires there are not 10 people to the square kilometer. While Germany, with her 135 inhabitants to the square kilometer, is supposed to have room for them!\\n\\n2. They assure us: We cannot take them unless Germany is prepared to allow them a certain amount of capital to bring with them as immigrants.\\n\\nFor hundreds of years Germany was good enough to receive these elements, although they possessed nothing except infectious political and physical diseases. What they possess today, they have by a very large extent gained at the cost of the less astute German nation by the most reprehensible manipulations.\\n\\nToday we are merely paying this people what it deserves. When the German nation was, thanks to the inflation instigated and carried through by Jews, deprived of the entire savings which it had accumulated in years of honest work, when the rest of the world took away the German nation\\'s foreign investments, when we were divested of the whole of our colonial possessions, these philanthropic considerations evidently carried little noticeable weight with democratic statesmen.\\n\\nToday I can only assure these gentlemen that, thanks to the brutal education with which the democracies favored us for fifteen years, we are completely hardened to all attacks of sentiment. After more than eight hundred thousand children of the nation had died of hunger and undernourishment at the close of the War, we witnessed almost one million head of milking cows being driven away from us in accordance with the cruel paragraphs of a dictate which the humane democratic apostles of the world forced upon us as a peace treaty. We witnessed over one million German prisoners of war being retained in confinement for no reason at all for a whole year after the War was ended. We witnessed over one and a half million Germans being torn away from all that they possessed in the territories lying on our frontiers, and being whipped out with practically only what they wore on their backs. We had to endure having millions of our fellow countrymen torn from us without their consent, and without their being afforded the slightest possibility of existence. I could supplement these examples with dozens of the most cruel kind. For this reason we ask to be spared all sentimental talk. The German nation does not wish its interests to be determined and controlled by any foreign nation. France to the French, England to the English, America to the Americans, and Germany to the Germans. We are resolved to prevent the settlement in our country of a strange people which was capable of snatching for itself all the leading positions in the land, and to oust it. For it is our will to educate our own nation for these leading positions. We have hundreds of thousands of very intelligent children of peasants and of the working classes. We shall have them educated -  in fact we have already begun and we wish that one day they, and not the representatives of an alien race, may hold the leading positions in the State together with our educated classes. Above all, German culture, as its name alone shows, is German and not Jewish, and therefore its management and care will be entrusted to members of our own nation. If the rest of the world cries out with a hypocritical mien against this barbaric expulsion from Germany of such an irreplaceable and culturally eminently valuable element, we can only be astonished at the conclusions they draw from this situation. For how thankful they must be that we are releasing these precious apostles of culture, and placing them at the disposal of the rest of the world. In accordance with their own declarations they cannot find a single reason to excuse themselves for refusing to receive this most valuable race in their own countries. Nor can I see a reason why the members of this race should be imposed upon the German nation, while in the States, which are so enthusiastic about these \"splendid people,\" their settlement should suddenly be refused with every imaginable excuse. I think that the sooner this problem is solved the better; for Europe cannot settle down until the Jewish question is cleared up. It may very well be possible that sooner or later an agreement on this problem may be reached in Europe, even between those nations which otherwise do not so easily come together.\\n\\nThe world has sufficient space for settlements, but we must once and for all get rid of the opinion that the Jewish race was only created by God for the purpose of being in a certain percentage a parasite living on the body and the productive work of other nations. The Jewish race will have to adapt itself to sound constructive activity as other nations do, or sooner or later it will succumb to a crisis of an inconceivable magnitude.\\n\\nOne thing I should like to say on this day which may be memorable for others as well as for us Germans: In the course of my life I have very often been a prophet, and have usually been ridiculed for it. During the time of my struggle for power it was in the first instance the Jewish race which only received my prophecies with laughter when I said that I would one day take over the leadership of the State, and with it that of the whole nation, and that I would then among many other things settle the Jewish problem. Their laughter was uproarious, but I think that for some time now they have been laughing on the other side of their face. Today I will once more be a prophet: If the international Jewish financiers in and outside Europe should succeed in plunging the nations once more into a world war, then the result will not be the Bolshevization of the earth, and thus the victory of Jewry, but the annihilation of the Jewish race in Europe!\\n\\n...The nations are no longer willing to die on the battlefield so that this unstable international race may profiteer from a war or satisfy its Old Testament vengeance. The Jewish watchword \"Workers of the world unite\" will be conquered by a higher realization, namely \"Workers of all classes and of all nations, recognize your common enemy!\"')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_document = loader.load()\n",
    "text_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-05T07:52:07+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-05T07:52:07+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Report Latest.pdf', 'total_pages': 27, 'page': 0, 'page_label': '1'}, page_content='Business Intelligence Traineeship - KPMG Luxembourg\\nSubmitted in Partial Fulfillment of the Requirements for the Degree of\\nMasters in Data Science\\nby\\nSahil Mohammad\\n0220993945\\nDepartment of Mathematics\\nUNIVERSITY OF LUXEMBOURG\\nLuxembourg\\n29th August, 2024'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-05T07:52:07+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-05T07:52:07+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Report Latest.pdf', 'total_pages': 27, 'page': 1, 'page_label': '1'}, page_content='Abstract\\nThis report provides a detailed explanation of the project completed as part of my internship\\nwithin the Business Intelligence team at KPMG Luxembourg. This project focused on enhancing\\ndata accessibility and to provide a holistic view of the usage of different reports across KPMG.\\nBeing an audit firm, it was really crucial to take care of the data privacy standards and hence, data\\nanalysis and management was a crucial part of the project. To accomplish the same, SQL stored\\nprocedures were utilized to ensure efficient and accurate handling of information. As mentioned,\\nsignificant aspect of the project was the implementation of data privacy measures. Sensitivity\\nto regulatory requirements lead to decisions to mask certain kind of data and to randomize the\\nother thereby balancing data utility with confidentiality. The report also emphasizes the role\\nof visualization in data reporting. This project utilized PowerBI to create visualizations and\\nreports after data preparation was done. Additionally, the project highlighted the automation\\nof deployment pipelines using Azure DevOps. This platform facilitated the easy deployment of\\nreports across different environments, which eliminated the need of any manual processes. As\\na member of the BI team, this internship provided firsthand experience in leveraging technical\\nskills for database management, data privacy compliance, reporting, and automated deployment\\nprocesses. The knowledge and experienced gained here contributes to a deeper understanding of\\neffective data management practices within a professional environment, which lays emphasis on\\ntheir significance in driving informed business decisions.\\n1'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-05T07:52:07+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-05T07:52:07+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Report Latest.pdf', 'total_pages': 27, 'page': 2, 'page_label': '2'}, page_content='Contents\\n1 Introduction 3\\n1.1 Mission and Values . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\\n1.2 Organization Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\\n1.3 Focus Areas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\\n1.4 Business Intelligence Team . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n1.4.1 BI Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\\n1.4.2 STAR . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\\n2 Project Discussion 7\\n3 Implementation 9\\n3.1 Data Preparation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\\n3.1.1 Masking Procedure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\\n3.1.2 Randomization Procedure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\\n3.2 Reporting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\\n3.2.1 myBI Dashboard . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\\n3.2.2 Time Window . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\\n3.2.3 Workspaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\\n3.2.4 Data Model Refreshes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\\n3.2.5 Project F500 - Yearly Tax Declaration . . . . . . . . . . . . . . . . . . . . . . . . 17\\n3.3 Development of Deployment Pipelines . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\\n4 Conclusion and Future Work 24\\n4.1 Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\\n5 References 25\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-05T07:52:07+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-05T07:52:07+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Report Latest.pdf', 'total_pages': 27, 'page': 3, 'page_label': '3'}, page_content='1 Introduction\\nKPMG is a brand name when it comes to Audit, Tax and Advisory. One of the renowned Big4 firms,\\nit is located centrally in Luxembourg City’s Kirchberg district. There is a lot of diverse workforce with\\nover 1800 employees belonging to 70 different nationalities. This very fact makes it a great place to\\nmeet new people, and also gives a sense of belonging to every employee. The firm serves a diverse\\nclient base looking over important areas such as asset management, alternative investments, banking,\\ninsurance, corporates, and the public sector.\\n1.1 Mission and Values\\nKPMG Luxembourg operates under the umbrella of KPMG International Limited, which is a globally\\nintegrated network of independent firms. Their presence is known in 143 countries all over the world\\nand KPMG International takes pride in a huge workforce comprising of over 273,000 partners and em-\\nployees. Each member firm, just like KPMG Luxembourg, operates autonomously, providing specific\\nand tailored services that can cater to the local clientele. But at the same time, they need to adhere\\nto the global standards and maintain all aspects of excellence and integrity.\\nKPMG Luxembourg offers a comprehensive list of services designed to address the evolving needs\\nof its clients. In addition to audit, tax, and advisory services, the firm provides specialized expertise in\\nareas such as financial services, including asset management and banking, as well as insurance and pub-\\nlic sector consulting. This multidisciplinary approach allows KPMG Luxembourg to offer integrated\\nsolutions that enhance operational efficiency, regulatory compliance, and strategic decision-making for\\nits clients.\\n1.2 Organization Structure\\nFigure 1: Organization Structure\\nFigure 1 describes the organizational structure of KPMG. It emphasizes on the core service areas\\nand how they integrate along with each other in addition to providing comprehensive support functions\\nto deliver services to the clients. In addition to the core areas, there are also specific sectors mentioned\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-05T07:52:07+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-05T07:52:07+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Report Latest.pdf', 'total_pages': 27, 'page': 4, 'page_label': '4'}, page_content='within the core areas which emphasize on the importance and the various task that each core area\\nundertakes within the organization. The structure can be broken down into the following areas as\\nexplained in Figure 1 :\\n• Tax : KPMG’s Tax services provide detailed tax compliance, advisory, and planning solutions\\ndesigned to help clients get through the complexities of national and international tax regulations,\\nsince KPMG is a global brand. By staying updated with the constantly evolving tax laws and\\npolicies, KPMG ensures that their clients remain compliant while enhancing their tax positions.\\nThis involves preparing and filing tax returns, advising on tax implications of various business\\ndecisions, and developing strategies to minimize tax liabilities. We will see in the upcoming\\npages, how I worked on a project which helps clients with F500 tax returns. The Tax team also\\noffers specialized services such as transfer pricing, tax risk management, and dispute resolution\\nto address specific client needs, ensuring a holistic approach to tax management.\\n• Audit : KPMG’s Audit services are basic building blocks to ensure financial transparency and\\nregulatory compliance for their clients. The Audit team conducts both statutory and internal\\naudits, while examining financial statements and internal controls to provide assurance on their\\naccuracy and reliability. Through strict testing and verification processes, KPMG helps clients\\nmaintain confidence in their financial reporting, while building trust among stakeholders. The\\naudit process not only detects and prevents financial irregularities but also identifies opportunities\\nfor improving internal controls and operational efficiencies. Theses audits are not only for the\\nclients but also within the teams. There have been audits on the BI team as well, which take\\nnotice of how the team handles senstitive data, and if KPMG policies are being respected.\\n• Advisory : KPMG’s Advisory services offer a wide range of strategic support to help clients\\nimprove and grow their businesses. They provide business advice, help make operations more\\nefficient, assist with financial restructuring, and develop policies. KPMG’s advisory team works\\nclosely with clients to understand their specific challenges and goals, creating customized solutions\\nthat drive big changes. Whether it’s improving supply chains, upgrading IT systems, or handling\\nmergers and acquisitions, KPMG has the knowledge and insights needed to make smart decisions.\\nTheir comprehensive approach ensures that clients can keep up with changing market conditions,\\ngrow sustainably, and stay competitive.\\n• Business Services : The Business Services provide essential support functions that for the basis of\\nthe operational success of the clients. These services include financial management, IT solutions,\\nand other critical operational functions that ensure smooth business operations. By offering\\nspecific solutions in areas such as finance, HR, and IT, KPMG helps clients improve efficiency,\\nreduce costs, and focus on their main areas of work. All other teams apart from Tax, Audit and\\nAdvisory, fall under Business Services. The Business Intelligence team as expected, falls under\\nbusiness services too. Instead of dealing with clients directly, these teams mostly help the other\\nteams by providing solutions for enhancing and optimizing their work.\\n1.3 Focus Areas\\nThese core areas focus on quite a lot of sectors which are mentioned below :\\nBanking and Insurance: In addition to providing tailored services for financial institutions, this\\nsector plays a very important role in ensuring compliance with both national and international regula-\\ntions. By meticulously navigating regulatory landscapes, KPMG supports banks and insurance firms\\nin adhering to stringent guidelines that govern their operations. By offering comprehensive risk assess-\\nment and mitigation strategies, KPMG enables financial institutions to enhance their risk management\\nframeworks and operational resilience. This proactive approach not only safeguards the interests of\\nclients but also strengthens their competitive edge in a dynamic and highly regulated financial envi-\\nronment.\\nAlternative Investments: These core areas perform valuation, due diligence, and advisory ser-\\nvices for a wide range of investments such as private equity, real estate, debt funds etc. They evaluate\\npotential investment opportunities and conducting financial due diligence. As far as market analysis is\\nconcerned, they perform the actual analysis, property valuation, and transaction support. Moreover,\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-05T07:52:07+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-05T07:52:07+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Report Latest.pdf', 'total_pages': 27, 'page': 5, 'page_label': '5'}, page_content='they structure and manage debt funds and perform financial modeling and risk assessment for large-\\nscale infrastructure projects.\\nAsset Management: This function deals with the investment strategy development, portfolio man-\\nagement, and performance analysis. Which means, that the areas crafting strategies to optimize returns\\nbased on market conditions and client objectives. They monitor and adjust investment portfolios to\\nmeet client goals. Additionally, evaluating the performance of various asset classes and investment\\nfunds is one of the many tasks for the core sectors.\\nCorporate Industry and Public Sector: Last but not least, another very important function\\nof the sectors is to provide Business advisory, audit, and compliance services for corporate clients and\\npublic sector organizations. It comes with strategic planning, operational efficiency improvements, and\\nfinancial restructuring. The audit services on the other hand conduct statutory audits and internal\\naudits to ensure compliance and transparency. With the public sector outlook, KPMG looks forward\\nto advising clients on policy development, public finance management, and program implementation.\\n1.4 Business Intelligence Team\\nThe Business Intelligence (BI) team within the Business Services subsidiary at KPMG Luxembourg\\nplays an essential role in enhancing data-driven decision-making across the firm’s key sectors: Audit,\\nTax, and Advisory. The MyBI project is central to the BI team’s mission, focusing on optimizing\\nBI processes and workflows using Azure DevOps. This project aims to streamline data management\\ntasks, from extraction and transformation to report generation and deployment, creating efficient and\\neasy to understand solutions. The primary objective of the BI team is to develop a comprehensive\\ndata warehouse. This data warehouse serves as a virtual, integrated, and clean data environment that\\nmeets the firm’s extensive reporting and analytical needs. The data warehouse ensures stakeholders\\nhave access to reliable and ready-to-use data, which helps them in making informed decisions. Built\\non an on-premise architecture, it is important for the BI team to adhere to the compliance regulations\\nat KPMG and also to manage the license costs effectively.\\nIn the Audit sector, the BI team enhances data accuracy and consistency by meticulously extracting\\nand transforming data from various sources. This ensures auditors have reliable data for comprehensive\\ncompliance reporting and risk assessment, using advanced tools and analytics to identify and mitigate\\npotential risks. The team’s strict guidelines to maintain data quality helps uncover discrepancies, and\\nensure that financial reports reflect true financial performance. The BI team’s efforts contribute to\\nthe robustness of audit practices, ensuring that clients adhere to regulatory requirements, maintain\\ntransparency, and build trust with investors and regulatory bodies. By integrating data from multiple\\nsystems and standardizing it, the BI team streamlines the audit process, reducing the time and effort\\nrequired for manual data reconciliation. We will see further how the different sources are integrated\\ntogether in the BI environment.\\nWithin the Tax focus area, the Business Intelligence (BI) team leverages advanced data analytics\\nto provide deep insights into tax compliance and planning. We produce detailed and comprehensive\\ntax reports that help clients optimize their tax strategies and gain a clear understanding of their tax\\npositions and obligations. These reports are tailored to highlight potential tax savings, identify risks,\\nand offer strategic recommendations to improve overall tax efficiency. The BI team plays a crucial\\nrole in compliance monitoring by continuously analyzing tax-related data to ensure adherence to both\\nlocal and international regulations. For the Advisory sector, the BI team delivers strategic, data-driven\\ninsights that enable advisory teams to craft well-informed plans for clients. The team’s ability to track\\nand analyze key performance indicators (KPIs) provides essential guidance for client advisory services.\\nBy leveraging data analytics and visualization techniques, the BI team helps uncover trends, identify\\nopportunities, and pinpoint potential challenges within the client’s business. The KPIs are important in\\nany report that the BI teams creates. These KPIs give much needed information around any indicator\\nwhich maybe used for a better performance in any areas that are lacking behind.\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-05T07:52:07+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-05T07:52:07+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Report Latest.pdf', 'total_pages': 27, 'page': 6, 'page_label': '6'}, page_content='1.4.1 BI Architecture\\nFigure 2: BI Team Architecture\\nThe architecture of the BI team is designed in a way to ensure efficient data management and\\nreporting across KPMG Luxembourg. The whole process begins with collection of data from various\\nsources, including file transfers, CRM systems, SQL Databases, Excel and CSVs and more importantly,\\nKPMG’s internal centralized system : STAR.\\n1.4.2 STAR\\nAt KPMG, the centralized repository, often referred to as STAR, is the heart of the company’s data\\noperations. It serves as a very comprehensive and a huge data center, centralizing critical information\\nabout staff, clients, jobs, engagements, and work in progress (WIP). This repository is very important\\nand highly necessary for KPMG’s operations, as it supports a wide range of functions from time entry\\nand client billing to scheduling and reporting.\\nThe data center contains detailed records of KPMG’s staff, including their roles and assignments.\\nThis information is the baseline and important for managing client engagements and ensuring that the\\nright resources are allocated to the right tasks. The repository also organizes clients into groups, re-\\nflecting the diverse and complex nature of KPMG’s client relationships. For instance, a client group like\\n’University of Luxembourg’ might consist of various client accounts such as ’University of Luxembourg\\n- Belval’, ’University of Luxembourg - Kirchberg’, and ’University of Luxembourg - Limpetsberg’. All\\nof these client accounts will have different charge account IDs but within the same client group. This\\nhierarchical structure allows for precise management and billing of client engagements. Also, in the\\nend you can calculate the total amount that was spent for a client group in general.\\nWithin the STAR system, each client is associated with specific jobs and engagements. These records\\nare essential for tracking the progress of work and ensuring that each task is accounted for. The WIP\\ncomponent of the repository provides real-time insights into the ongoing work, detailing which client\\nand charge account each engagement belongs to. This information is crucial for accurate time entry in\\nthe timesheet system, ensuring that all billable and non-billable hours are correctly attributed.\\nOne of the core functionalities of the repository is its role in client billing. Clients are billed based on\\ndifferent charge type IDs, which categorize the nature of the work performed. These charge types in-\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-05T07:52:07+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-05T07:52:07+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Report Latest.pdf', 'total_pages': 27, 'page': 7, 'page_label': '7'}, page_content='clude categories such as Planning, Profit/Loss, Non-Chargeable IT, Client Meetings, and many others.\\nBy categorizing work in this way, the system ensures that clients are billed accurately and transpar-\\nently for the services they receive. STAR accurately tracks which client’s account was charged, what\\ntype of charge (identified by the charge type ID), who performed the work (based on staff ID), and\\nthe nature of the job. This level of detail ensures that every transaction is thoroughly documented,\\nproviding a clear and comprehensive record for both internal audits and client billing. For example, if\\na staff member works on a planning task for ’University of Luxembourg - Kirchberg’, the repository\\nwill log the staff ID, the specific client account, the charge type ID for planning, and the details of the\\njob performed.\\nBeyond billing, the STAR repository is also a vital tool for scheduling and planning. It provides\\ncapabilities for planning resources and scheduling tasks, helping KPMG manage its workforce effi-\\nciently. By using the repository to track WIP and plan future engagements, the company can optimize\\nits operations and ensure that all projects are completed on time and within budget. The data stored in\\nthe STAR repository is not just for operational purposes; it also supports comprehensive reporting and\\nanalytics. By analyzing the data, KPMG can gain valuable insights into its operations, identify trends,\\nand make informed decisions. This analytical capability is crucial for maintaining high standards of\\nservice and continuously improving the company’s performance while providing client satisfaction.\\nNow coming back to the architecture as shown in Figure 2, the raw data is first processed in the\\nstaging area using Microsoft SQL Server Integration Services (SSIS), where it undergoes cleansing and\\nharmonization to ensure accuracy and consistency. Accuracy is essential because it guarantees that\\nthe data reflects the true values and scenarios that are being analyzed, which is critical for making\\ninformed decisions. Consistency ensures that the data from different sources conforms to the same\\nstandards and formats, enabling meaningful integration and reliable analysis across various datasets.\\nIf we do not ensure accuracy and consistency, any analysis or reports generated could be misleading,\\nor probably would lead to incorrect conclusions and potentially wrong business decisions.\\nOnce this is done, the next step is to move the cleansed data to the main data warehouse, which is also\\nmanaged by the Microsoft SQL Server. In this central repository, the data is stored in a structured\\nformat, facilitating easy access for further analysis. The BI team uses Microsoft SQL Server Analysis\\nServices (SSAS) to create data models that organize the data into structures that make querying data\\nvery efficient. In order to ensure security of the data and compliance, an implementation of Role-Based\\nSecurity (RLS) is implemented, which controls access based on user roles.\\nIn the architecture, the final step involves serving the processed data through various reporting tools.\\nPowerBI Server (on-premise) and PowerBI Service (cloud) provide very robust platforms for internal\\nand easy to scale, cloud-based reporting respectively. Additionally, Excel is used for traditional data\\nanalysis, and paginated reporting is available for generating other reports in a detailed manner. The\\narchitecture can also integrate additional sources like SharePoint and Azure DevOps, which leads to\\nenhanced collaboration and overall management of the project. As a whole, this BI Architecture sup-\\nports KPMG Luxembourg’s core areas of Audit, Tax, and Advisory by delivering clean, integrated,\\nand reliable data which helps the various teams in the firm to make decisions based on reports which\\nhighlight nothing but factual information from the data. Thus, providing data-driven decisions and\\nhigh quality services.\\n2 Project Discussion\\nThe first phase in the project, like in a typical data analysis life cycle was comprehensive data prepa-\\nration. This was a very important step which was aimed at ensuring compliance with the strict data\\nprivacy standards that are essential for KPMG, given the sensitive nature of the data that is handled\\nby an audit firm. The process began with classifying the entire database into different privacy levels.\\nSensitive data such as names, emails and client names were identified for special handling to protect\\nall confidentiality. For instance, certain columns containing the sensitive information were masked\\n/ anonymized to preserve the structure while obscuring the actual data. This approach was highly\\nessential for the maintenance of data privacy without compromising the integrity that was needed for\\nany further report development or analysis. In cases where masking the data would cause the data\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-05T07:52:07+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-05T07:52:07+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Report Latest.pdf', 'total_pages': 27, 'page': 8, 'page_label': '8'}, page_content='Figure 3: Project Overview\\nto be not much useful for reporting, such as with names and emails, randomization techniques were\\nemployed. This allowed the data to remain useful for analytical purpose while ensuring that the data\\nor the report does not reveal any real information.\\nNow, in order to streamline and enhance the data preparation process, SQL stored procedures were\\nutilized extensively. These procedures provided automation of various tasks, such as data masking and\\nrandomization, which in turn ensures consistency and reduces the chances and probabilities for any\\nmanual errors. By making use of the SQL procedures, the project achieved much better data handling\\nand enhanced the performance in the data preparation phase of the life cycle. Additionally, these\\nprocedures facilitated faster data processing, allowing for more efficient data analysis and reporting.\\nThe automation also freed up valuable time for the team, enabling them to focus on more complex and\\nstrategic tasks rather than routine data preparation. Moreover, the use of stored procedures improved\\nscalability, making it easier to manage larger datasets as the project grows, and ensured that best\\npractices and data governance standards were consistently applied throughout the process.\\nOnce the data was prepared, the next phase focused a lot more on the report development. The\\nname of the report created is “myBI Usage” which provides a comprehensive overview of the utiliza-\\ntion of the various BI reports within KPMG Luxembourg. The report includes a lot of key metrics,\\nincluding the total number of subscriptions, reports and executions, offering a holistic view of the report\\nusage across the organization. There are various sections in the report which allow the users to access\\ndetails for various information. Different visualization were utilized which makes the report easy to\\nunderstand while conveying a lot of information at the same time. Another dashboard was developed\\nfor the F500 - Yearly tax declaration. The object is to generate a financial statement that clients will\\nappend to myGuichet as part of their yearly tax declaration process. There were two reports developed\\nfor this project. The first report was developed for the partners and it would track the tax declaration\\nprocess through its various stages by connecting to SharePoint online lists. It would also represent the\\npercentage of time that was spent on each stage for all the CTRs (Company Tax Returns). The second\\nreport was for Timeframe calculations between different stages of the process. This report would cal-\\nculate the time spent between different stages to identfiy and forecast any potential bottlenecks in the\\nprocess. This was achieved by using the last modified information from the SharePoint lists. Further\\nexplanations on these reports will be provided in later sections.\\nPost report development, the final phase of the project involved the implementation of Deployment\\nPipelines using Azure Devops. This phase was critical to ensure that the developed reports can be\\neasily and efficiently deployed across various environments without any manual effors. By automating\\nthis deployment process, Azure DevOps pipelines ensure consistency and reliability, which minimizes\\nthe risk of any potential errors and highly reduces the time required for deployments. The deployment\\npipelines were configured to handle the transition and flow of reports from development to the produc-\\ntion environment, which helps the project to conform and adhere to the best practices of Continuous\\nIntegration and Continuous Deployment (CI/CD), which is highly sought after in Data Engineering as\\nwell.\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-05T07:52:07+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-05T07:52:07+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Report Latest.pdf', 'total_pages': 27, 'page': 9, 'page_label': '9'}, page_content='As a member of the Business Intelligence team at KPMG Luxembourg, the project not only enhanced\\ntechnical skills but also explained the importance of effective data management practices in driving\\ninformed business decisions. This report contains sections on each of the phases of the project which\\nare mentioned above. In these sections we aim to tackle and explain in much more detail each phase,\\nwhile highlighting the importance as well as the steps that were undertaken to implement all the same\\nin the project.\\n3 Implementation\\nThe section above gave a brief overview of the whole project cycle and the steps that were followed in\\nthe completion of the internship project. This section deals with detailed explanation on the various\\nsteps involved and tries to explain the flow of the project in a detailed manner. The steps involved are\\nas follows :\\n3.1 Data Preparation\\nThe data warehouse used by the BI team resides in SQL Server which acts as a robust and highly\\navailable database that supports a lot of applications and offers features like point-in-time restorations.\\nThis data warehouse, named ’DWH’ in SQL Server Management Studio (SSMS), serves as the cen-\\ntral repository for consolidating and organizing data from various sources, making it a crucial tool for\\nanalytical and reporting purposes. The data warehouse is carefully designed to provide a structured en-\\nvironment for both data storage and retrieval, ensuring that data is easily accessible and well-organized.\\nAt the center of this data warehouse are its 235 tables, which are essential for storing transactional\\ndata, dimensional data, and metadata. These tables include various dimension and factual tables, each\\nholding a wide range of information necessary for comprehensive data analysis.\\nThe preparation of data for projects primarily involved the classification of these various columns\\n[1]. Given the large number of tables and added to that, the high number of columns within them, this\\nclassification process can be quite cumbersome and time-consuming. However, understanding why this\\nclassification is necessary is crucial before diving into the process itself [3]. Classification is essential\\nfor several reasons. First, it ensures that data is organized in a logical and consistent manner, which\\nis critical for efficient data retrieval and analysis. By categorizing columns based on their type and\\npurpose, the process of querying and analyzing data can be streamlined, making it easier to generate\\naccurate and insightful reports.\\nIn addition, classification aids in maintaining data quality and integrity. By clearly defining the types\\nand constraints of each column, the BI team can implement validation rules and checks that prevent\\nany data entries with potential errors and ensure consistency across the data warehouse. This is par-\\nticularly important in a large-scale environment like the data warehouse, where the volume of data can\\nmake manual quality control not much useful and impractical. Overall, while the process of classifying\\ncolumns in a vast data warehouse like DWH may be challenging, it is a vital step that lays the foun-\\ndation for effective data management and analysis. Before getting to the process of classification, and\\nhow it was actually performed on the data warehouse, it is important to understand why classification\\nis actually required.\\nWhat is Classification ?\\nClassification is the process of categorizing data based on its sensitivity and importance. Here we\\nassign labels or tags to the data to indicate its level of confidentiality [3]. This complete process starts\\nwith organizing and listing out the data, which may include tables and various columns that need to\\nbe identified. Once the data is organized, the importance of each attribute is to be decided. This is\\ndone by considering several factors like how risky is the data, what would the business impact be if\\nthat data is accessible, and who should have access to it.\\nThis step involves defining clear classification criteria that align with organizational policies and regula-\\ntory requirements. These criteria help in systematically assessing the sensitivity of data elements. After\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-05T07:52:07+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-05T07:52:07+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Report Latest.pdf', 'total_pages': 27, 'page': 10, 'page_label': '10'}, page_content='setting the criteria, each data item is evaluated and classified accordingly. High-sensitivity data might\\ninclude personal identifiable information (PII), financial records, or proprietary business information,\\nwhile low-sensitivity data might include publicly available information. It’s crucial to regularly review\\nand update the classification as the data and its context can change over time. Implementing data\\nclassification policies helps in mitigating risks by ensuring that sensitive data is adequately protected\\nand only accessible to authorized personnel. Training and awareness programs are also essential to\\nensure that employees understand the importance of data classification and adhere to the guidelines.\\nWhy is Classification needed ?\\nThe primary reason why classification is needed is to make sure that KPMG data is protected ac-\\ncording to its level of sensitivity. It is clear the not all data is equally sensitive and some information\\nmay pose a higher risk if it is exposed or compromised. Based on these differences, we used three\\ndifferent sentivity labels, which are as follows:\\n• General : This information can be shared with external partners, as required.\\n• Confidential : Sensitive business data that could cause damage to the business if shared with\\nunauthorized people.\\n• Confidential - GDPR : Sensitive data containing personal information associated with an indi-\\nvidual, that could be misused.\\nThe Classification procedure as mentioned above, begins with identifying the columns that contain\\nsensitive information. For this, we need to go through all the columns in each table, in each schema\\nand decide if they should be marked sensitive or not. This information is then updated in the SQL\\nServer Management Studio. Once the columns are identified and labelled, there are two processes that\\nneed to be carried out, which are as follows:\\n• Masking : This process involved replacing sensitive data with anonymized values while preserving\\nthe data format [2]. It ensures that no one can read the data once it has been masked. In our\\ncase, we mask sensitive information by replacing the characters with ’x’. For example, a person’s\\nemail such as john.doe@kpmg.lu will become joxxxxxxoe@kpmg.lu.\\n• Randomization : Here, we replace sensitive data with entirely new and random values. Unlike\\nmasking which hides the data, randomization replaces actual data with fake/simulated data. This\\nis very helpful in reporting as we will have some values to display instead of words that have been\\nmasked with x’s. It is clear that when randomization is done, a mapping table is required to\\nmake a link between the random data and the real data that is being randomized or else the\\ninformation would be lost.\\nMoreover, in the case of new table being added to the database, the sensitive columns in the new table\\nshould also be added to the existing Data Classification in SQL Server Management Studio and masking\\nand randomization queries should be run again so that the new sensitive data is also protected. Once\\nall the data has been classified with the respective sensitivity labels, the stored procedure for masking\\nof the data can be run on it. These processes are explained in much more detailed in the next subsection.\\n3.1.1 Masking Procedure\\nThe process is carried out with the help of a stored procedure designed to protect sensitive information\\nby masking data in the tables. It uses data cursors to sequentially process each row, extract table,\\ncolumn and schema names along with the column details from the system’s extended properties table.\\nThe system’s extended properties contains the information that has been classified in the SSMS. All\\nthe columns and the senstivity label that has been provided to that particular column is present in the\\nsystem tables. SQL cursor is nothing but a database object that is particularly useful for operations\\nthat require row-level-processing, such as performing calculations and updates on each individual row.\\nThe procedure iterates through each row, using the cursor to fetch and process data from the staging\\ntables. For numeric fields, it assigns a fixed number, for date fields, it sets a standard date, and for\\nstring fields, it either reduces the string to a single character or masks part of it with ’x’s. It is to\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-05T07:52:07+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-05T07:52:07+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Report Latest.pdf', 'total_pages': 27, 'page': 11, 'page_label': '11'}, page_content='be noted that the procedure skips masking certain fields like names of people, their emails, windows\\nlogins and the client names so that randomization is possible on them later. Since there are cases where\\nthe PowerBi report has to show some names and data, it only makes sense to replace the information\\nwith fake names to allow reporting to be done successfully. This procedure, as a whole ensures that\\nsensitive information remains confidential and secure during the staging process. Once the masking /\\nanonymization is done, randomization of all the columns that have been skipped from masking has to\\nbe done for reporting purposes as explained above. This process is explained in the subsection below.\\n3.1.2 Randomization Procedure\\nThe process of randomization is important because masking hides all the data and we would not be left\\nwith values that can be used for reporting purposes. However, randomization is not as direct as the\\nmasking procedure as here we need to first create a mapping table that would link together the new\\nrandom values with an actual staff member or a client. We create two mapping tables. The first one\\ncontains information related to staff and the second one is for clients. To create the tables, we make\\nuse of the following IDs :\\n• KPMGGPID : These are unique identifiers for staff and hence, can be used to link actual staff\\nwith the randomized names.\\n• GISID : The clients have these unique identifiers which are used for mapping purpose with the\\nnewly generated fake business names.\\nHowever, we need to replace names of almost 12,000 staff members with a fake name. Therefore, creat-\\ning a single name manually for each and every staff was not possible. This process was also automated\\nusing SQL [4]. Instead of thinking of every individual name, a list of 200 fake names was utilized which\\ncan be easily generated from a fake name generator present online. These names were in the format\\nof ‘FirstName’ and ‘LastName’ which was inserted into a table. After inserting these names in a table\\ncalled ‘RandomNameBank’, a cross join was performed between the FirstName and LastName columns\\nto give a huge number of combinations of fake names.\\nThe CROSS JOIN is used to show every possible combination between two or more sets of data.\\nWe can do a cross join with more than two sets of data, allowing for comprehensive combinations.\\nCross Joins are typically done without join criteria, unlike the inner, left, and right joins which require\\na condition to match rows. A Cross Join is also known as the Cartesian Join as it is nothing but the\\nCartesian product of the two sets of data. This means every row from the first set is combined with\\nevery row from the second set. It can be done between different tables or on two different columns of\\nthe same table, which is what we used in the project. For example, if you have a table with n rows, a\\ncross join would produce n*n results. Hence, since the RandomNameBank contains 200 rows, a cross\\njoin gave us 40.000 results. This extensive combination of data pairs can be particularly useful for\\ngenerating all possible pairs or scenarios that need to be tested or analyzed.\\nIn our project, the cross join was utilized to create a comprehensive list of staff names by combin-\\ning first names and last names from two separate columns within the RandomNameBank. By doing\\nso, we ensured that we had a varied and exhaustive set of potential names for use in simulations or\\nanonymization processes. The benefit of using a cross join in this context is that it provides a straight-\\nforward and efficient way to generate all possible name combinations, which can then be filtered or\\nused as needed for further data processing tasks.\\nMoreover, while cross joins can be powerful, they should be used judiciously, as the resulting dataset\\nsize grows exponentially with the size of the input tables. This can lead to performance issues if not\\nmanaged properly. Therefore, understanding the implications and ensuring that the system can han-\\ndle the resultant data load is crucial when implementing cross joins in any project. Proper indexing\\nand optimization techniques should be applied to handle large datasets effectively, making the process\\nsmoother and more efficient.\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-05T07:52:07+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-05T07:52:07+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Report Latest.pdf', 'total_pages': 27, 'page': 12, 'page_label': '12'}, page_content='Figure 4: Working of a Cross Join\\nFollowing the generation of fake names, the next critical phase involves creating a mapping table.\\nThis table serves as a centralized repository linking each staff member’s unique KPMGGPID which\\nwas mentioned above, to their corresponding fake name. Alongside the fake names, the mapping table\\nincludes additional attributes such as an email ID generated specifically for the fake name and a Win-\\ndows login ID associated with the staff member. This comprehensive approach ensures that each staff\\nmember is effectively connected to their anonymized identity across various systems and applications.\\nWith the mapping table in place, a stored procedure is employed to systematically identify and ran-\\ndomize names across the database. The procedure scans through all tables containing columns with\\nnames that match specific criteria, typically involving the term ’name’ in their column headers [5].\\nUpon identifying such columns, the procedure retrieves the actual names stored within and initiates a\\nlookup process. During the lookup, the procedure cross-references each actual name with the staff table\\nto retrieve the corresponding KPMGGPID. This step is very important as it enables the procedure to\\npinpoint the exact staff member associated with each instance of an actual name within the database.\\nBy using the mapping table created before, the procedure then retrieves the corresponding fake name\\nlinked to the identified GPID.\\nOnce the fake name is identified, an update statement is executed to replace the actual name with\\nthe fake name across the database. Given the huge size of the database, this process might be a bit\\ntime consuming as it replace all the instances of a person’s name with the fake name accross all the ta-\\nbles. This automated process ensures uniformity and consistency in anonymizing personally identifiable\\nand any information related to staff members throughout all data sets and tables. By systematically\\nreplacing actual names with fake names, this project and process mitigates the risk of exposing sensitive\\ninformation while adhering to strict data protection regulations and compliance standards. Moreover,\\nsince there still exists a name instead of masked name, the fake name can be now utilised in the gener-\\nation of reports and at least the reports will now have information to display instead of masked data.\\nThe following figure explains the name generation, mapping table creation, and the lookup processes\\nin an efficient way:\\nThis way, the names, windowslogins and emails for the staff members have been randomized [6].\\nAdditionally as mentioned before, the client names were to be randomized as well. In this case, the\\nclient names were replaced by fake business names generated in a way similar to the fake person names.\\nThe only challenge with generating a mapping table for client names was that the client information\\nwas present in two different tables. Also, as mentioned before, the client GISIDs were used to map\\nan actual client with a fake generated client. These IDs had to be combined from two different tables\\nusing the union operation while taking care of using only unique IDs and skipping any client that was\\nrepeated in the two data sources.\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-05T07:52:07+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-05T07:52:07+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Report Latest.pdf', 'total_pages': 27, 'page': 13, 'page_label': '13'}, page_content='Figure 5: Lookup and Randomization\\nOnce the mapping table was ready for the Client information, the stored procedure could be cre-\\nated. Just like person names, Client name was also searched for in all the tables and the corresponding\\nID was selected from the Client Source table [7]. Once the ID was retrieved, the same ID was searched\\nfor in the Mapping table and the corresponding fake client name was obtained. Then a simple update\\nstatement changed the actual client name with the fake name [8].\\n3.2 Reporting\\nOnce the data preparation was done, the main task of the BI team is to provide dashboards and reports\\nwhich in turn provide valuable insights to the different teams that requested the report. To prepare the\\ndata, analyse it, and to provide valuable findings from it is one of the most important aspects of Data\\nAnalysis life cycle. In section 3.1 we discussed how the data was prepared and senstive data was masked\\nand randomized for efficient reporting purposes. In this section, we will discuss more about the reports\\nthat were created post data preparation and the information that they provided. The development\\nprocess involved creating comprehensive and interactive dashboards using PowerBI. Since KPMG has\\na partnership with Microsoft, the tools used here are all provided by the latter and hence, PowerBI\\nwas utilised for reporting purposes. Moreover, PowerBI provides a lot of features and capabilities along\\nwith a user friendly interface which helps translate complex data sets into understandable insights.\\n3.2.1 myBI Dashboard\\nThe primary dashboard, as shown in the myBI Usage report, provided a holistic view of report usage\\nacross KPMG. This report used about 18 different tables, some of which were taken from the SQL\\ndatabase and a couple of them were created in PowerBI. To connect these tables together, special\\ncare had to be taken to join the correct fields together. Moreover, the cardinality of the join was an\\nimportant factor in joining the tables together. The most commonly used cardinalities are as follows\\n[14] :\\n• 1:1 Cardinality : In a one-to-one (1:1) relationship, each entity from the first set is associated\\nwith exactly one entity from the second set, and vice versa. This is perfect choice in case of\\njoining unique ids from one table to another table with unique IDs. Thus, for each ID, there is a\\nunique ID which maintains the 1:1 order.\\n• 1:* Cardinality : In a one-to-many (1:*) relationship, a single entity from the first set can be\\nassociated with multiple entities from the second set but each entity from the second set is\\nassociated with only one entity from the first set. For example in case of the myBI reports, there\\nare multiple reports in a folder on the PowerBI server. Moreover, in the opposite direction, a\\nsingle folder holds multiple reports which maintains 1-many relationship.\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-05T07:52:07+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-05T07:52:07+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Report Latest.pdf', 'total_pages': 27, 'page': 14, 'page_label': '14'}, page_content='Once the correct data was pulled into the report, the dashboard was designed with several key features\\nand sections which are mentioned below :\\n• Activity Summary : This section provided a snapshot of overall report usage, highlighting the\\nfrequency and distribution of report executions. This section helped identify peak usage times\\nand potential bottlenecks in report access.\\n• Workspaces : Power BI workspaces are collaborative environments within the Power BI service\\nwhere users can work together on dashboards, reports, datasets, and dataflows. This section\\nprovides information about different workspaces to show how various departments or teams were\\nutilizing the BI tools. This allowed for a better understanding of departmental engagement and\\ncollaboration.\\n• Report and User Lookup : These features enabled users to search for specific reports and users,\\nproviding details about which report was executed recently and the time details about each\\nexecution on each report. This was particularly useful for troubleshooting and for identifying\\npower users or training needs.\\n• Report Inventory and Catalog History : A comprehensive list of all available reports and their\\nfolder details is maintained in this view. It also shows the last modification details for each report.\\n3.2.2 Time Window\\nThe above list briefly mentions the various types of information present in the myBI report. However\\nthe whole report’s execution tracking feature controls all the visuals present in it. This feature is de-\\nsigned to provide users with a easy to use and a very interactive way to monitor the frequency of report\\nusage over various time periods. Users have the flexibility to select a time frame such as the past 7, 30,\\n60, or 90 days—using a slicer tool on the dashboard [15]. This simple selection mechanism allows the\\ndashboard to dynamically update and display the number of times reports have been executed within\\nthe chosen period, offering valuable insights into report usage trends. Moreover, as mentioned before,\\nthe slicer selection controls all the other visuals as well to show the results from a given time window.\\nTo implement this time window functionality, a specialized date table is created in PowerBI. This\\ntable is provided with markers that indicate whether each date falls within the last 7, 30, 60, or 90\\ndays. These markers are like filters, that enable the visuals to accurately count only the relevant exe-\\ncutions for the selected time frame. For example, if a user selects the 7-day period, the dashboard will\\ncount and display only those executions that occurred within the last 7 days. Similarly, selections for\\n30, 60, and 90 days will adjust the displayed data accordingly.\\nThere are additional functionalities added to the date table which help in a lot of other visuals. These\\nfunctionalities were implemented by addition of various additional columns, such as day of the week,\\nmonth, year, and flags for weekends. This detailed implementation allows for more detailed data\\nanalysis, since this report is for monitoring the executions of various reports, it was important for\\nimplementation of various time window details. Separate measures are defined for each time period,\\nensuring that the correct calculations are performed based on the user’s selection. These measures use\\nthe markers from the date table to filter and count the report executions accurately.\\nThis information is used on the Activity Summary page which tracks the number of executions of\\nthe report according to the days and the time. Using this feature we can know how many reports are\\nbeing executed and when is the busiest time for the executions. On the prod server, this report shows\\nthat the busiest days are Mondays and Fridays basically because they are the first and the last days\\nof the week and hence there is more activity. This single page contains a lot of information like the\\npercentage of success and failed executions, along with the above explained details by the week. Addi-\\ntionally, there is another visual called the ’Selected Execution Count’ that combines all the individual\\ncalculations and displays the appropriate count which explains the count of executions along with the\\nnumber of distinct users that ran the executions. This line chart provides quite a lot of information\\nwhile using the Date table that was created earlier.\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-05T07:52:07+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-05T07:52:07+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Report Latest.pdf', 'total_pages': 27, 'page': 15, 'page_label': '15'}, page_content='3.2.3 Workspaces\\nPower BI workspaces [16] are a fundamental part of the Power BI service. They are collaborative envi-\\nronments where teams can work together on Power BI content, such as dashboards, reports, datasets,\\nand dataflows. There are mainly two different types of workspaces:\\n• Personal Workspace\\n• Shared Workspace\\nConsidering the first one, as the name suggests it is a personal workspace available to every Power BI\\nuser. It is intended for individual use and is not typically shared with others. Users can create and store\\npersonal dashboards, reports, and datasets here. Secondly, a shared workspace is the one that are col-\\nlaborative workspaces where multiple users can contribute. They are ideal for teams working on shared\\nprojects. In shared workspaces, users can collaborate on the development of reports and dashboards,\\nensuring consistency and enabling integration and combination of individual work. Workspaces have\\nadministrators that can control access to the workspace by assigning different roles to users. Common\\nroles include Admin, Member, Contributor, and Viewer, each with varying levels of permissions. The\\nsharing of data and organization of dashboards etc can be done on the workspace itself. Once this is\\ndone, the reports and dashboards can be published to the workspace. After they are published, they\\ncan be shared with other users, either within the workspace or even in a broader way by sharing it\\naccross the organization.\\nThe workspace owner plays an important role in managing and maintaining the workspace. The\\nowner is responsible for creating the workspace, defining its purpose, and taking care of its overall\\nadministration. They manage access by assigning roles to other users, such as Admins, Members,\\nContributors, and Viewers, each with different permissions. The workspace owner also takes charge\\nof content management, ensuring that dashboards, reports, datasets, and dataflows are organized and\\nup-to-date. Any issues with the workspace be it access related or performance related, needs to be\\naddressed by the owner or by someone who as relevant access. A page in the report has been im-\\nplemented which shows all the details regarding the all the workspaces present in KPMG. There are\\ndifferent states that a workspace can reside in. These states are as follows :\\n• Orphan Workspaces : Orphan workspaces occur when a workspace no longer has an active owner.\\nThis can happen if the owner leaves the organization or their account is deactivated. Without\\nan owner, the workspace lacks proper oversight, which can lead to issues in managing content\\nand permissions. Organizations typically need to reassign ownership to another user to ensure\\nthat the workspace continues to function effectively and that its contents remain accessible and\\nmanageable.\\n• Suspended Workspaces : Suspended workspaces are those that have been temporarily deactivated,\\nwhich mostly is because they have been inactive for quite a long time. In this state, users cannot\\naccess the workspace or its contents until the suspension is lifted. The suspension could be a\\nresult of not adhering to data governance policies, misuse, or other administrative reasons. Once\\nthe issues are resolved or the necessary actions are taken, the workspace can be reactivated and\\nnormal operations can resume.\\nThe ”Workspace Status” page shows all the details about workspaces and the number of workspaces\\nin these particular states. It categorizes workspaces based on their orphaned status which is : active,\\ninactive, suspended, and orphaned and presents a summary count for each category. Additionally,\\nit lists workspaces by their respective owners, showing the number of workspaces managed by each\\nindividual. This page of the report highlights key metrics, including the total number of workspaces\\nand unknown workspaces. A lot of detailed information for each workspace is displayed, including\\nworkspace names, statuses, owners, departure dates, and functions which is nothing but to which sector\\nof KPMG does the workspace belong to. These sectors were discussed in a lot of detail in section 1.2\\nof this internship report. These enables efficient monitoring and management of workspaces, ensuring\\nup-to-date information on workspace usage and ownership. The breakdown by orphaned status, WO\\n(work order) status, and ”Left On” dates further aids in identifying and managing workspace utilization\\nand transition of owners. This report was very useful at an occassion when the BI Team received an\\nemail from KPMG global to delete all the workspaces that have been inactive for a long time. This\\n15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-05T07:52:07+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-05T07:52:07+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Report Latest.pdf', 'total_pages': 27, 'page': 16, 'page_label': '16'}, page_content='particular page of the dashboard was used and hence this particular feature of the report was very\\nhelpful.\\n3.2.4 Data Model Refreshes\\nThe Power BI Data Model Refreshes View is a critical tool within the ”myBI Dashboard” report,\\nwhich offers a real-time visibility into the status of data model refreshes. This view provides users with\\ndetailed insights into the refresh process for various Power BI reports. It includes information such as\\nthe directory where each report is stored and the exact title of the report, which helps the team to\\nquickly locate and manage their reports. The view categorizes reports by type, such as interactive or\\npaginated, and provides detailed descriptions of refresh schedules, the last run times which is nothing\\nbut the last time a refresh occurred on a report. Users can also see the status of the most recent refresh\\nattempt, whether it succeeded or failed, along with any error messages that might have been generated.\\nThe timestamp of the last refresh operation is also displayed to indicate how recent the data is, and\\nthe event type classification helps anyone in the team working on an issue to understand the context\\nin which the refresh was triggered, whether it was scheduled or manually initiated.\\nThe Power BI Reporting Services - Subscription Status provides detailed insights into the status of\\ntimed subscriptions for Power BI reports. It includes information on the report name, subscription\\nname, event type, last run date and time, last status, and a description of the last run. This status\\nreport is crucial for understanding the execution history and current state of subscriptions, indicating\\nif they are successful, pending, or disabled. Every day at 9 AM, a subscription status email is sent,\\nsummarizing the latest refresh statuses and subscription outcomes. This email ensures that stake-\\nholders are aware of the most recent updates and any issues that need attention. For instance, if the\\n”DQJobs” report refreshes successfully at 3 AM on July 19, 2024, this success would be reflected in the\\ndaily email, providing a comprehensive update on the data availability and report health. Together, the\\nPower BI Data Model Refreshes View and the Daily 9 AM Subscription Status Email ensure a robust\\nmonitoring framework. The real-time insights provided by the Data Model Refreshes View allow users\\nto address issues as they arise, while the daily email offers a broader perspective on refresh activities\\nand issues over time. This combination enables the team to maintain up-to-date and accurate reports\\nand manage their business intelligence operations more efficiently.\\nThe daily subscription status email that is sent every morning at 9 AM, summarizes the refresh out-\\ncomes and provides the team with an everyday update on the health of the PowerBI Server. In case\\na report refresh is failed, it needs to be investigated as to what caused the failure and any issues need\\nto be resolved so that the refresh can be done again and the report is functional as as result. Any\\nof these reports might be used by any team within KPMG and a failed report means the other team\\ncannot work on their tasks. Hence, this email serves as a regular checkpoint, highlighting any critical\\nissues that need attention. By combining these two tools, the organization ensures both immediate,\\ndetailed monitoring and periodic, summarized updates. This approach in a dual manner enhances\\ntransparency, reliability, and the overall efficiency of the data reporting process. The team can rely\\non the email for daily updates and use the PowerBI view for in-depth analysis when necessary. This\\nsystem supports proactive data management and helps maintain the integrity of the reports provided\\nto any of the other teams and users.\\nWhile this report focuses on these pivotal aspects of the myBI Dashboard, it is important to ac-\\nknowledge that the dashboard consists of a much a broader range of information and features designed\\nto enhance data management and reporting efficiency. The information and features discussed in this\\ninternship report represent a summary of the most impactful elements, but the full dashboard offers\\nadditional functionalities that can provide even deeper insights and support more detailed analyses.\\nAll in all, the ”myBI Dashboard” is an invaluable resource for optimizing our data reporting processes\\nand ensuring the effective management of our reporting assets. The features outlined in this report\\nhighlight its capacity to deliver critical information and support informed decision-making, reinforcing\\nits role as a cornerstone of our data management strategy.\\n16'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-05T07:52:07+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-05T07:52:07+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Report Latest.pdf', 'total_pages': 27, 'page': 17, 'page_label': '17'}, page_content='3.2.5 Project F500 - Yearly Tax Declaration\\nProject F500 aims to facilitate the generation of a comprehensive financial statement that clients will\\nuse as part of their annual tax declaration process. This financial statement is a critical component of\\nthe yearly tax filings and is designed to be appended to the myGuichet platform, an essential tool for\\nmanaging and processing tax-related submissions in Luxembourg. The primary objective of this project\\nis to streamline and automate the tax declaration process, ensuring that the financial statements are\\naccurate, timely, and compliant with relevant tax regulations. By achieving this, the project not only\\nsimplifies the tax declaration for clients but also enhances the overall efficiency and reliability of the\\ntax reporting process.\\nIn the context of the project, integrating with the MyGuichet platform is really important. MyGuichet\\nis Luxembourg’s official digital platform that provides a centralized, user-friendly interface for citizens,\\nresidents, and businesses to access and manage a wide range of government services online. Launched\\nto enhance efficiency and accessibility, it allows users to submit documents, request information, and\\nperform various administrative tasks electronically. The task for BI team is to create the reports as\\nrequired by the Tax Team. However, connecting to MyGuichet and using the reports for filling in de-\\ntails on the portal is not the scope for the BI team. By ensuring that the financial statement produced\\nand the insights provided by the F500 reports are valid and appropriate, the project aims to provide\\nthe tax team and then the clients that the tax team is dealing with, a streamlined and efficient means\\nof fulfilling their tax obligations. This integration is designed to enhance the accuracy of submissions\\nand reduce the manual effort involved in preparing tax documents.\\nThe reports are designed to be a comprehensive tool for monitoring and managing the tax decla-\\nration process. It includes visualizations and metrics that track key performance indicators (KPIs),\\nsuch as the number of declarations processed, the status of each declaration, the time taken on each\\nstage and any potential issues or delays. By utilizing PowerBI features, the report can present complex\\ndata in an easily understandable format, allowing users to make data driven decisions and take timely\\nactions. One of the core objectives of Project F500 is to enhance the efficiency of the tax declaration\\nprocess. This is done with the help of two reports which meticulously monitor and manage the timing\\nand status of various Client Tax Returns (CTRs) throughout their lifecycle. The project is designed to\\nensure that each CTR is tracked efficiently, from its initiation to its final submission, thereby enhancing\\nthe overall management of tax returns within the organization.\\nWhat is a CTR, and how it relates to the F500 Report ?\\nA Client Tax Return (CTR) is a crucial document that represents a tax return specific to an en-\\ntity within a client’s organization. Each CTR corresponds to a unique entity and encapsulates the\\nfinancial information and tax details corresponding to that entity. In this context, a CTR is not just\\na generic tax return but a detailed report that is modified to the specific needs and circumstances of\\neach client entity. Project F500 is centered around the detailed monitoring of the timing and status\\nof these CTRs all of these CTRs for different clients and the entities for these clients. This involves\\nseveral key activities:\\n• Tracking Progress: The project tracks the progress of each CTR through various stages of the\\ntax declaration process. This includes monitoring when a CTR is created, when it is reviewed,\\nand when it is finally submitted. By maintaining a close watch on these timelines, Project F500\\nensures that no delays occur and that each CTR is processed within the required timeframes.\\n• Status Updates: The status of each CTR is continuously updated and recorded. This involves\\ncapturing various status indicators such as ‘In Progress,’ ‘Under Review,’ ‘Pending Submission,’\\nand ‘Submitted.’ Regular updates help in maintaining an accurate and up-to-date view of the\\nCTR’s lifecycle, facilitating better management and timely intervention if needed.\\n• Entity-Level Reporting : Each CTR is associated with a specific entity within a client’s organi-\\nzation. Project F500 ensures that the returns are not only tracked at the client level but also at\\nthe entity level. This means that the project provides detailed insights into the status and timing\\nof CTRs for each individual entity, allowing for a more granular and precise management of tax\\nreturns.\\n17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-05T07:52:07+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-05T07:52:07+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Report Latest.pdf', 'total_pages': 27, 'page': 18, 'page_label': '18'}, page_content='• Integration with Systems: To effectively track and manage CTRs, Project F500 integrates with\\nvarious systems and platforms. This includes leveraging data from SharePoint Online Lists to\\nmonitor changes and updates, as well as using reporting tools to provide insights into the timing\\nand status of each CTR. Integration with these systems ensures that the project has access to\\naccurate and comprehensive data, which is crucial for effective monitoring.\\n• Identifying Bottlenecks: The main aspect of Project F500 is its ability to identify and address\\npotential bottlenecks in the CTR process. By analyzing the timing and status data, the project\\ncan pinpoint areas where delays may occur or where additional support may be needed. This\\nproactive approach helps in resolving issues before they impact the overall tax return process.\\n• Improving Efficiency : By focusing on the timing and status of CTRs, Project F500 aims to\\nenhance the efficiency of the tax return process. This includes streamlining workflows, reduc-\\ning delays, and ensuring that all CTRs are processed in a timely manner. Improved efficiency\\ntranslates to better management of tax returns and a smoother experience for clients.\\nAccuracy is another fundamental goal of Project F500. Ensuring that the financial statement reflects\\nthe true financial position of the CTR and the correct stage for a client entity is critical to identify\\nthe exact bottleneck. In addition to tracking the progress of the tax declaration process, Project F500\\nalso includes a time calculation component. This aspect of the project involves calculating the time\\nspent between different steps in the tax declaration process. The goal is to identify and forecast poten-\\ntial bottlenecks or delays that could impact the overall efficiency of the process. To achieve this, the\\nproject leverages information from the version history available in SharePoint Online lists. This version\\nhistory provides a detailed record of changes and updates made to the tax declaration data, including\\ntimestamps for each change. By analyzing this data, the project team can calculate the duration spent\\nat each stage of the process and identify any areas where improvements can be made.\\nReport Development :\\nOne of the particularly interesting steps in the F500 project was to integrate with a more dynamic\\nand collaborative platform that acts as a data source rather than a traditional static database [17].\\nSharePoint Online serves not just as a plain old data repository but also as a collaborative tool where\\nmultiple users can interact with their data in real-time. The organization or the repository that\\nthe data resides under can also provide role-based accesses and this ensures data privacy standards\\nwithin KPMG. Since it has access requirements, the complexity and the project development time was\\nincreased as it required handling various aspects like user permissions, version history, and live data up-\\ndates. The process of establishing a connection to SharePoint involved leveraging its APIs and handling\\nits unique data structures, which differ significantly from the relational tables typical of SQL databases.\\nOut of the many data sources supported by PowerBI, it also allows connection to a SharePoint Online\\nlist as well. Once the required access was provided, a connection to the SharePoint List was made\\nusing PowerBI directly and this gave access to all the tables present under the list. The required tables\\nwere then pulled in, which are as follows:\\n• Business Processes: This table contains information about the various business processes involved\\nin the tax declaration process. It includes information about the Client, the entity, title of the\\nCTR, the status and more information about Partners and Managers assigned to the same.\\n• EventStore: This table is the most important of all and contains information like the CTR name,\\nthe current status that the CTR is in, the time and date of modification and the person assigned\\nto that particular CTR.\\n• User Information List: This table contains information about users involved in the tax declaration\\nprocess, including user IDs, names, and roles. It helps in associating changes and updates with\\nspecific individuals.\\nThere are specific relationships needed between all the tables for various visualization purposes, the\\ntables mentioned above are imported into Power BI, where they are used to create a data model that\\nsupports the reporting and analysis needs of the project.\\n18'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-05T07:52:07+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-05T07:52:07+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Report Latest.pdf', 'total_pages': 27, 'page': 19, 'page_label': '19'}, page_content='Report 1 : Partners\\nAs mentioned before, there are two reports developed as part of this Project 500. The first one being\\nthe Partners Report has been developed to serve needs of KPMG Partners, which helps them in getting\\na comprehensive overview of all the Client Tax Returns across the organization. The report is designed\\nto offer partners a deep insight into the status and progression of each CTR, which is combined with\\na detailed client and entity information.\\nWhen a partner accesses the dashboard, the system loads data related to all CTRs under their team\\nand control. This data encompasses associated details for each client and their respective entities,\\nproviding a full view of the ongoing processes. The report does not only display data simply, but it\\nserves as an important tool for partners to monitor progress, identify potential bottlenecks, and en-\\nsure timely completion of tax-related tasks. As mentioned before, the data source is the SharePoint\\nOnline list. The most important requirement from the Partners Report is an interactive bar chart,\\nwhich graphically represents the percentage of the number of CTRs in each stage of the process. This\\nvisualization helps partners in quickly assessing the distribution of work and identifying areas that may\\nrequire additional resources or attention. The bar chart is designed to be interactive, allowing partners\\nto drill down into specific stages for more detailed information.\\nA detailed table in the Partners Report gives a complete list of key points, designed so that we can\\neasily see how the Client Tax Return (CTR) is doing. This table is carefully designed to contain a\\nnumber of important columns: the ”Client” column, which states the name of the client for which the\\nCTR is being processed, and the ”Entity” column, which reveals the exact entity inside the organi-\\nzation of the client. The ”CTR” column is the code that identifies each Client Tax Return that is\\nshared to track and reference individual cases. The ”Status” column denotes the current stage of the\\nCTR within the tax declaration process, so it lets partners know what stage each return has reached\\ncurrently. Furthermore, this table has the ”KPMG Operator Name,” who is the KPMG employee in\\ncharge of dealing with the records, and the ”KPMG Manager Name,” who is the manager in charge\\nof the process respectively. The ”KPMG Partner Name” column connects the partner with the client\\nand the CTR, making sure that the person responsible is accountable and clear in managing the pro-\\ncesses. The ”Modified Date” column displays the date and time of the last modification to the CTR,\\nso we get a timeline of the updates and changes. Finally, the ”Year” column indicates the tax year\\nfor which the CTR is effective and, it helps in the organization and the historical tracking of returns.\\nThis configuration along with the details mentioned guarantee that partners can instantly retrieve and\\nanalyze the data they need, which in turn quickens the decision making and the execution of tasks as\\nwell as supports the firm’s tax-related activities being carried out efficiently.\\nReport 2 : CTR TimeFrame Calculation\\nThis is a specialized report designed for the Tax team, focusing on the details and stages involved\\nin the Client Tax Return (CTR) processes. The primary objective of this report is to provide a com-\\nprehensive view of all CTRs, which shows their progress through various stages and calculate the time\\nspent on each stage. This detailed information is very important for the Tax team to monitor efficiency,\\nidentify potential bottlenecks, and optimize the tax declaration workflow. The first report discussed\\nabove calculates the percentage of CTRs in each stage while this report calculates the time spent on\\neach stage for each CTR.\\nThe report is designed with an easy to understand user interface that allows users to effortlessly\\nnavigate through the data. Upon accessing the dashboard, the user from tax team will be presented\\nwith an overview of all CTRs, categorized by their current stages. This includes everything from initial\\nsubmission to the final approval stages. The dashboard provides a holistic view of the time spent on\\neach stage of the CTR, enabling the Tax team to understand the overall efficiency of the process. One\\nof the key features of the dashboard is the ability to drill down into specific stages. By selecting a\\nparticular stage, users can view the cumulative time spent on that stage across all CTRs. This func-\\ntionality is essential for pinpointing stages that may be causing delays or require additional resources,\\nthereby facilitating improvements in the workflow.\\n19'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-05T07:52:07+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-05T07:52:07+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Report Latest.pdf', 'total_pages': 27, 'page': 20, 'page_label': '20'}, page_content='Similar to the first report, the data source is a SharePoint Online List. The primary table used\\nfrom this datasource is the EventStore table. This table is important and used as it records each event\\nor action taken during the CTR process, including timestamps and details of changes, making it an\\nimportant source for tracking the progression and time allocation of each CTR. A slicer is implemented\\nto display the various stages of all CTRs currently in process. This slicer acts as a filter, allowing users\\nto focus on specific stages by simply selecting them. Once a stage is selected, the report dynamically\\nupdates to display the total time spent on that stage, providing immediate visual results. This inter-\\nactive feature not only enhances the user experience but also allows for quick identification of stages\\nthat may require attention due to processing times that are longer than expected.\\nIn addition to the detailed view of individual Client Tax Returns, the report also provides multi-\\nple key performance indicators that offer a bigger and broader insight into the overall tax declaration\\nprocess. These KPIs include the total number of CTRs, which helps to know the volume of work and\\nthe wide range of client engagements for the tax team. The total number of clients shown in the dash-\\nboard provides an overall idea of the client base covered, giving a sense of the report’s reach. Moreover,\\nthe report tracks the time spent on the stage currently selected in the slicer, allowing the user from the\\ntax team to focus on specific aspects of the process. This feature is particularly useful for identifying\\nareas that may require additional resources or improvements. Finally, the report highlights the stage\\nconsuming the most amount of time, offering useful insights into potential bottlenecks or inefficiencies\\nwithin the process of filing tax returns for the clients. This information can be very useful for the Tax\\nteam in prioritizing process improvements and optimizing their workflow.\\nAn important step in development of this report is the time spent spent on each and every stage\\nof the CTR. The SharePoint list has the modified time corresponding to the CTR but not the time\\nspent. Therefore this is something that had to be calculated explicitly using PowerBI measures. To\\nachieve this, the code first looks for the timestamp of the current event and identifies the business\\nprocess it belongs to using the unique Business Process ID. It then searches through the dataset to\\nfind all events associated with the same business process. Among these events, it filters out those that\\noccurred after the current event, focusing only on those that happened earlier. This filtering helps us\\nin ensuring that the analysis remains relevant and corresponds only to the sequence of events leading\\nup to the current stage. Once we find all the stages for one particular CTR, the code calculates the\\nmaximum timestamp from this group. This timestamp represents the latest stage that occured before\\nthe current stage within the same business process. By finding and recording this previous event, the\\ncode can then simply subtract the timestamp of the current event and the most recent previous event.\\nThis time difference is put in a new column of the table. The visual then uses this column to produce\\nthe visual and display the time spent on each stage.\\nAfter the time taken at each stage is calculated, another measure is created which would find out\\nthe stage out of all the currently available stages that takes the maximum time to complete. This stage\\nis then populated in the KPI which was discussed above which calculates the stage on which most time\\nis being spent. This information calculation and providing the same on the report is crucial for several\\nreasons. It helps in analyzing the efficiency of the process, identifying any delays or bottlenecks that\\nmay exist between different stages. For example, if there is a significant gap between two events, it\\ncould indicate an area where the process is slowing down, suggesting a need for improvement or fur-\\nther investigation. Additionally, understanding these time intervals is essential for accurate reporting,\\nallowing stakeholders to track the progress and status of various processes effectively.\\nThis is how the reports are designed to be a comprehensive tool for monitoring and managing the\\ntax declaration process. By utilizing the various PowerBI features, the reports present complex data in\\nan easily understandable format, allowing users to make data driven decisions and take timely actions.\\nOne of the core objectives of Project F500 was to enhance the efficiency of the tax declaration process.\\nThis is done with the help of the two reports which were discussed above that monitor and manage\\nthe timing and status of various CTRs throughout their lifecycle. The project was designed to ensure\\nthat each CTR is tracked efficiently, from its initiation to its final submission, thereby enhancing the\\noverall management of tax returns within KPMG. As mentioned before, these reports are used by the\\ntax team to fulfill the tax obligations and then finally fulfill the obligations on myGuichet for the client.\\n20'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-05T07:52:07+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-05T07:52:07+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Report Latest.pdf', 'total_pages': 27, 'page': 21, 'page_label': '21'}, page_content='3.3 Development of Deployment Pipelines\\nDeployment pipelines are very useful tools in the software development and the BI environments, as\\nthey help in the smooth transition of reports for BI purposes or any code when it comes to Software\\nDevelopment, across different stages and environments such as Development (DEV), User Acceptance\\nTesting (UAT), and Production (PROD)[18]. The Pipelines were developed along with a Data Engi-\\nneering team and implemented for the BI team for two types of reports, which are mentioned as follows\\n:\\n• SSRS Reports: SQL Server Reporting Services (SSRS) reports are a feature of Microsoft’s SQL\\nServer that provide a detailed solution for creating, deploying, and managing reports. These\\nreports are designed to be accessible across the entire organization. SSRS reports can be either\\npaginated or mobile, that caters to different reporting needs.\\n• PowerBI Reports: Power BI reports are dynamic and interactive visual representations of data\\ndesigned to provide insights and facilitate decision-making. They allow users to explore data\\nthrough various visualizations like charts, graphs, and maps, enabling easy identification of trends\\nand patterns. This makes them ideal for self-service business intelligence, empowering users to\\ncreate, share, and collaborate on data insights efficiently.\\nThese pipelines play a crucial role in ensuring that reports are developed and deployed in a structured,\\nefficient, and secure manner. Utilizing deployment pipelines, especially with Azure DevOps, offers\\nseveral advantages, including streamlined processes, reduced manual effort, and enhanced consistency\\nacross different environments. Earlier without the pipelines, a BI Engineer had to go through the\\nmanual effort of setting connection strings and uploading the reports to all the different environments.\\nHowever, now with Deployment pipelines, these reports can be deployed as soon as they are uploaded\\nto the repository. Azure DevOps is a suite of development tools and services that supports the entire\\napplication lifecycle, from planning and development to testing and deployment. It is very useful and\\nimportant in setting up and managing deployment pipelines for both Power BI and SSRS reports.\\nAzure DevOps integrates easily with Git to provide the required version control and management in a\\nCI/CD Environment.\\nIn this setup, Git repositories serve as the central place for all report development and updates. When\\na new report or an update is committed and pushed to the Git repository, Azure DevOps triggers a\\nbuild pipeline. This automated process ensures that any changes are consistently and immediately\\ncompiled, tested, and prepared for deployment. There are two different kind of pipelines created. One\\nis for PowerBi reports and the other is for SSRS Reports. The foundation of the pipeline lies in the\\nstructured architecture of the Git repository. The repository should be organized as follows:\\n− Project Reports\\n− Project Reports\\n− ∗ . r d l\\n− ∗ . r d l . data\\n− Project Reports . r p t p r o j\\n− Project Reports . s l n\\n− SQL\\n− . . .\\n− . g i t i g n o r e\\n− README.md\\nThis structure ensures that all necessary files, including the report definition files (.rdl), solution files\\n(.sln), and project files (*.rptproj), are systematically organized. The .gitignore file helps ignore any\\nunwanted files like build outputs or any other user specific settings, which can be ignored by simply\\nmentioning the extensions in the .gitignore file. Secure handling of credentials and sensitive information\\nis managed through the Azure KeyVault. Several information is passed to the keyvault like the service\\naccount username and password, the server URI and there should be a unique trigram assigned to each\\nrepository which uniquely identifies the artifacts deployed for that repository in the azure vault. Once\\nthe repository structure is present as mentioned above, another folder needs to be created at the root\\nfolder, called ’pipelines’ which will have all the files required for actually setting up the pipeline [19].\\nThese files are mentioned as follows :\\n21'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-05T07:52:07+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-05T07:52:07+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Report Latest.pdf', 'total_pages': 27, 'page': 22, 'page_label': '22'}, page_content='• Variables.yml: This file is like the vault for implementation of the pipeline. As the name suggests,\\nall the variables that are used throughout the pipeline are mentioned in this. It includes essential\\nproperties and key-value pairs like the folder name, which indicates the directory where the project\\nreport is stored. Additionally, it specifies the release folder name, which contains the report\\nproject files (.rptproj). The file also defines the artifact feed name, which is another repository\\nwhere the built reports are stored and versioned. Furthermore, it includes the KeyVault trigram,\\na unique identifier that links the deployment pipeline to the specific Azure KeyVault holding\\nsensitive credentials and server information, which was also mentioned above.\\n• Build-pipeline.yml: This configuration file outlines the steps for building the reports from the\\nsource files. It starts by defining the name and versioning strategy for the build, which helps\\nin maintaining a systematic version control which is related to the major. minor and any patch\\nupdates. The build-pipeline.yml file also specifies the source repository from which the pipeline\\nfetches the templates and scripts needed for the build process. It contains parameters for the\\nbuild configuration, such as whether the build should be in Debug or Release mode, and a package\\ndescription to briefly describe the update or changes made in that particular build. This is the\\nfile that builds the .rdl files and then deploys any of the generated artifacts to the artifact feed.\\nIt takes care of the correct versioning and the artifact is deployed along with the version that was\\nmentioned.\\n• Deploy-pipeline.yml: The deploy-pipeline.yml is a crucial part of automating the deployment\\nof SSRS reports within the MyBi project. This file contains the final steps necessary to take\\nthe reports, which have been built and versioned, and deploys them to the appropriate server\\nenvironments. There are several variables which specify key settings like feedName, projectFolder,\\ntargetFolder, and keyVaultTrigram. feedName refers to the specific artifact feed (such as mybi-\\nssrs-feed) where the reports are stored post-build. The projectFolder specifies the directory\\ncontaining the report files. targetFolder indicates where on the SSRS server the reports should\\nbe deployed. The keyVaultTrigram links the pipeline to Azure KeyVault, providing access to\\nnecessary credentials securely. If the keyvault has not been assigned, the deployments will not be\\npossible. All in all, the deploy-pipeline.yml picks up the artifact from the artifact-feed which was\\nuploaded earlier at the build-pipeline stage and deploys that artifact to the respective DEV, UAT\\nand PROD environments. Currently, the deployment to DEV is automated but for the other two\\nenvironments, it needs special approval from the Team Lead or Senior Engineers. This is to\\nprevent any unneccessary deployments to the Production environment and maintains a second\\nlayer of security.\\nFor a better understanding, the deployment workflow is explained below along with the figure:\\nFigure 6: Deployment Workflow\\nThe Figure 6 gives a detailed overview of how the Continuous Integration and Continuous Deployment\\nprocess works for a typical workflow. The various stages and components of the pipeline are explained\\nfurthermore in detail below:\\n22'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-05T07:52:07+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-05T07:52:07+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Report Latest.pdf', 'total_pages': 27, 'page': 23, 'page_label': '23'}, page_content='• Repository : The process begins with the Repository, which has all the source code, including\\nthe SSRS reports (.rdl files) and any associated project files. These repositories are hosted on\\nAzure Repos which is a Git Platform.\\n• Build Process (CI) : When a new code change is pushed to the repository, it triggers the Contin-\\nuous Integration (CI) pipeline. The CI pipeline performs several key functions:\\n– Build: The CI pipeline builds the project, compiling the SSRS reports and other components\\nwhich are relevant and necessary. This step ensures that the reports are correctly formatted.\\n– Push New Version: After the build, the compiled reports and other artifacts are packaged\\nand pushed to an Artifact Feed. This feed serves as a versioned storage for the build outputs,\\nmaking it easy to retrieve specific versions for deployment.\\n• Artifact Feed : The Artifact Feed acts as a central repository for the compiled reports. It stores\\ndifferent versions of the reports, allowing the team to track changes over time and deploy specific\\nversions as needed. This feed is particularly useful for ensuring consistency across different\\nenvironments, such as DEV, UAT and Prod.\\n• Continuous Deployment (CD) : The next phase involves the Continuous Deployment (CD) pipeline,\\nwhich automates the deployment of the reports to different environments. This stage is triggered\\nby the user or automatically based on certain conditions:\\n– Download Specific Version: The CD pipeline downloads the specific version of the reports\\nfrom the Artifact Feed that needs to be deployed.\\n– DEV Deployment: In the DEV environment, the pipeline creates the necessary folders and\\npushes the reports to the DEV Report Server. This environment is used for initial testing\\nand development purposes.\\n– Approval Gate: Before deploying to more critical environments like UAT or PROD, the\\npipeline includes an Approval Gate. This step requires manual approval, ensuring that\\nonly tested and validated reports are promoted to UAT/PROD environments. This was\\nmentioned before regarding the approval from a Senior Engineer or the Team Lead.\\n– UAT/PROD Deployment: Once approved, the pipeline creates folders and pushes the reports\\nto the UAT or PROD Report Server. This final deployment step ensures that the reports are\\navailable for end-users or final testing in the UAT environment before going live in PROD.\\nThis pipeline ensures a streamlined, automated process for deploying SSRS reports across different en-\\nvironments, significantly reducing the manual effort required and minimizing the risk of errors during\\ndeployment. By making use of Azure DevOps and Git repositories, the pipeline provides a detailed\\nand easy to maintain framework for managing report versions and ensuring consistency across servers\\nfor Development, Testing and Production. A similar procedure is followed for deploying the PowerBI\\nreports to the different servers [20]. The major difference is in the artifact feed. A separate feed is\\ncreated for PowerBI reports so that all the PowerBI artifacts are stored in one place and these are kept\\nseparately from the SSRS reports. The deploy-pipeline.yml file in this case looks for any artifacts with\\nthe extension of ’.pbx’ instead of ’.rdl’ that was the case in SSRS reports. Once the repository structure\\nis created and the files are added with the required paramaters, the pipeline can be set up explictly.\\nAzure Devops provides a ’Pipeline’ section which provides an easy way of setting up the pipeline once\\nthe required yml files are setup.\\nThe implementation of the deployment pipelines for SSRS and PowerBI reports in the project, not\\nonly brought consistency in the deployment workflow but also streamlined the process of moving the\\nreports throught the development, UAT and the production environment. As mentioned before, these\\ndeploymnets had to be done manually while taking care of all the versioning strategy, access rights and\\nconnection strings to all the different environments but now the pipelines have significantly reduced the\\nmanual effort that was typically associated with these deployments. By using Azure DevOps and the\\nGit repositories, the pipelines ensured an easy and efficient transition of reports from the development\\nto the deployment stages, which helps in maintaining consistency and reliability across the three dif-\\nferent environments. Additionally, this project enhanced inter-team communication and collaboration,\\nas the DevOps and the BI teams worked closely together to align the technical requirements with the\\nBusiness Intelligence needs. This collaboration was crucial in ensuring that the deployment pipelines\\nwere well-integrated with existing systems and workflows maintained by the DevOps team.\\n23'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-05T07:52:07+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-05T07:52:07+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Report Latest.pdf', 'total_pages': 27, 'page': 24, 'page_label': '24'}, page_content='4 Conclusion and Future Work\\nThe completion of the whole project at KPMG Luxembourg marks a significant achievement in en-\\nhancing the BI team’s data reporting and deployment capabilities, using multiple layers of innovation,\\nand development across various domains. The project began with a critical focus on data security and\\nprivacy, specially through the implementation of Data Classification using data masking and random-\\nization techniques. This was an essential step to make sure that any sensitive information belonging\\nto a client was protected throughout the report development lifecycle. By using a cross join operation,\\nwe generated a large number of fake names and randomized the data, providing the stakeholders with\\nrealistic data for testing and development without compromising any sensitive client data. This ap-\\nproach only made sure that the BI team follows the KPMG global policies.\\nOnce the data preparation was done, the focus was on the creation of comprehensive reports, which\\nwas crucial for both internal monitoring and client-specific deliverables. Among these reports, the\\nserver monitoring report provided valuable insights into the utilization of various reports across the\\norganization which helped in the management of BI infrastructure. Additionally, the F500 tax dec-\\nlaration reports were developed to assist the tax team in maintaining the tax compliance obligations\\nfor the clients. These reports provided detailed analytics on the tax declaration process, along with\\nthe tracking of each Client Tax Return (CTR) through the various stages. This not only made the\\nclient engagament and transparency better but also imporved the internal processes by highlighting\\nthe bottlenecks and any inefficient behaviour.\\nAfter securing the data and developing the reports, the final phase of the project involved setting\\nup deployment pipelines for all the reports within the BI space. These included all of the PowerBI\\nreports and the SSRS reports. By using DevOps, I established automated pipelines that significantly\\nreduced the manual effort that engineers were putting in for the deployment across different envi-\\nronments. The integration of Git Repositories through Azure Repos allowed for version control and\\ncontinuous integration/continuous deployment (CI/CD) processes. This automation not only improved\\nthe efficiency of the deployment process but also minimized the risk of errors, thereby ensuring that\\nthe end-users received the highest quality reports.\\nOverall, the project has not only advanced the technical capabilities but also enhanced the inter-team\\ncollaboration. This collaboration was important regarding the complexities in report development,\\ndata security and the deployment. I had to make sure that everything works well together in a well\\nintegrated manner.\\n4.1 Future Work\\nAs far as any future work is concerned, the foundations were laid down by this project and they will\\nserve as a good platform for further innovations and improvement in the BI team architecture. There\\nare some additional features that can be added to the project completed as part of the internship. This\\nis mentioned as follows:\\n• Data Classification: The Data Classification which was done at the very initial stage can be\\nenhanced further to support a script that would just ask for the columns needed to be masked,\\ninstead of going through the whole database and selecting the columns one by one.\\n• Monitoring Report: There can be real time alerting features implemented in the Monitoring\\nreport. For example, when there are multiple reports failing, the BI team can be notified about\\nthe same and any potential issues can be taken care of. This can be done with Power Automate.\\n• F500 Report : Currently, the different tables that are used for report development are filled in\\nwith details manually. There can be a script that would fill in the tables if the details have been\\nprovided in any of the tables used.\\n• Cloud Migration : As part of a future plan, all the PowerBI Reports need to be migrated to\\ncloud and hence, there is a need for gateways to connect to the on-premise data sources while\\nthe reports are in the cloud.\\n24'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-05T07:52:07+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-05T07:52:07+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Report Latest.pdf', 'total_pages': 27, 'page': 25, 'page_label': '25'}, page_content='• Pipelines for Cloud : The team does currently have PowerBI cloud reports, but there are no\\ndeployment pipelines for the same. These need to be created, much more now as there will be\\nmigration to the cloud. There is a requirement for a dedicated service account that would do the\\ndata model refreshes on the cloud.\\nThe deployment of on-premise PowerBI reports has paved way to extend the deployments to cloud.\\nThere is additional work required for connections by using specific service accounts and for refreshing\\nthe data over the cloud. Moreover, the initial data classification\\nThroughout my time on the project in KPMG, I was actively engaged in various team meetings,\\nincluding planning sessions, retrospectives and daily stand-ups. My contributions were often recog-\\nnized, particularly during retrospectives where my efforts in data masking and randomization, and\\ncollaborating with KPMG Netherlands team for resolving bugs in some other reports received com-\\nmendation. I was acknowledged for my ability to deliver high-quality results even when the deadlines\\nwere tight. Additionally, my internship tenure included comprehensive training sessions provided by\\nKPMG, which gave me much needed important knowledge related to data management and maintain-\\ning and adhering to the strict data privacy guidelines set by an audit firm as big as KPMG. Beyond the\\nmain project, I also contributed to smaller projects, where I implemented enhancements and resolved\\nissues with existing reports. Moreover, I was actively writing documentation for all my work and this\\nwas posted to the team’s internal Wiki which can be referred to in the future whenever needed. The\\nsix-month period in a global firm like KPMG not only broadened my experience but also deepened\\nmy understanding of the organization’s reporting needs and infrastructure. The extensive importance\\ngiven to any task related to handling of data was very intriguing to me. These experiences were invalu-\\nable in refining my technical skills and enhancing my ability to work effectively within a team-oriented,\\nagile environment.\\n5 References\\n1. Gupta, Rajendra. ”SQL data classification – Add sensitivity classification in SQL Server 2019”.SQL-\\nShack, Adding Sensitivity label-SQLShack.\\n2. Colley, Derek. ”Automatically Create and Anonymize Downstream Databases from Azure DB”.\\nmssqltips, Anonymizing the Databases - Azure DB.\\n3. Tripathy, Madhumita, et al. ”SQL Data Discovery and Classification”. Learn Microsoft, SQL -\\nData Discovery.\\n4. Hutmacher, Daniel. ”Fun with Random Names”. SQL Sunday, Random Name Generation.\\n5. Dave, Pinal. ”SQL SERVER – Find Column Used in Stored Procedure – Search Stored Procedure\\nfor Column Name”. SQL Authority, Searching column names in a stored procedure.\\n6. Dave, Pinal. ”SQL SERVER – Find Column Used in Stored Procedure – Search Stored Procedure\\nfor Column Name - Part 2”. SQL Authority, Searching column names in a stored procedure -\\nPart 2.\\n7. essentialSQL. ”What is a SQL Server Data Dictionary?”. CodeProject, Data Dictionary - Infor-\\nmation Schemas.\\n8. Pollack, Edward. ”Building a SQL Server data dictionary”. red-gate, Building Data Dictionaries.\\n9. ”SQL Server reporting services (SSRS) documentation”. ApexSQL, SSRS Documentation.\\n10. Neugebauer, Niko, et al. ”System catalog views (Transact-SQL)”. Learn Microsoft, Catalog\\nViews - Transact SQL.\\n11. K, Elazar, et al. ”SQL information protection policy - Microsoft for Cloud”. Learn Microsoft,\\nInformation Protection Policy.\\n12. Bar, Paulin, et al. ”Dynamic Management Views (DMVs)”. Learn Microsoft, Dynamic Manage-\\nment Views.\\n25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-05T07:52:07+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-05T07:52:07+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Report Latest.pdf', 'total_pages': 27, 'page': 26, 'page_label': '26'}, page_content='13. Murray, Scott. ”SQL Server 2012 Analysis Services (SSAS) DMVs”. mssqltips, SSAS DMVs.\\n14. Tanushree. ”Cardinality in DBMS”. GeeksforGeeks, DBMS Cardinality.\\n15. Myers, Miguel, et al. ”Create a relative date slicer and filter in Power BI”. Learn Microsoft,\\nRelative Date Slicer.\\n16. Bar, Paulin, et al. ”Workspaces in Power BI”. Learn Microsoft, PowerBI Workspaces.\\n17. Zhou, Rico. ”How to get version history from SharePoint”. Microsoft Fabric Community, Share-\\npoint Version History.\\n18. Wright, Josh, et al. ”Deployment jobs”. Learn Microsoft, Pipelines with Azure DevOps.\\n19. D, Steve. ”YAML schema reference for Azure Pipelines”. Learn Microsoft, YAML Reference.\\n20. Romano, Rui. ”Power BI Project (PBIP) and Azure DevOps build pipelines for validation”.\\nLearn Microsoft, Pipelines for PowerBI Projects.\\n26')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"Report Latest.pdf\")\n",
    "documents = loader.load()\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "\n",
    "loader = WebBaseLoader(web_paths=(\"https://jalammar.github.io/illustrated-transformer/\",),\n",
    "                       bs_kwargs= dict(parse_only = bs4.SoupStrainer(class_= (\"post\")))\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/'}, page_content='\\nThe Illustrated Transformer\\n\\nDiscussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n\\n\\n\\nUpdate: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they\\'ve evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n\\n\\nPopping open that Optimus Prime goodness, we see an encoding component, a decoding component, and connections between them.\\n\\n\\n\\nThe encoding component is a stack of encoders (the paper stacks six of them on top of each other – there’s nothing magical about the number six, one can definitely experiment with other arrangements). The decoding component is a stack of decoders of the same number.\\n\\n\\n\\nThe encoders are all identical in structure (yet they do not share weights). Each one is broken down into two sub-layers:\\n\\n\\n\\nThe encoder’s inputs first flow through a self-attention layer – a layer that helps the encoder look at other words in the input sentence as it encodes a specific word. We’ll look closer at self-attention later in the post.\\nThe outputs of the self-attention layer are fed to a feed-forward neural network. The exact same feed-forward network is independently applied to each position.\\nThe decoder has both those layers, but between them is an attention layer that helps the decoder focus on relevant parts of the input sentence (similar what attention does in seq2seq models).\\n\\n\\n\\nBringing The Tensors Into The Picture\\nNow that we’ve seen the major components of the model, let’s start to look at the various vectors/tensors and how they flow between these components to turn the input of a trained model into an output.\\nAs is the case in NLP applications in general, we begin by turning each input word into a vector using an embedding algorithm.\\n\\n\\n\\n\\n  Each word is embedded into a vector of size 512. We\\'ll represent those vectors with these simple boxes.\\n\\nThe embedding only happens in the bottom-most encoder. The abstraction that is common to all the encoders is that they receive a list of vectors each of the size 512 – In the bottom encoder that would be the word embeddings, but in other encoders, it would be the output of the encoder that’s directly below. The size of this list is hyperparameter we can set – basically it would be the length of the longest sentence in our training dataset.\\nAfter embedding the words in our input sequence, each of them flows through each of the two layers of the encoder.\\n\\n\\n\\n\\nHere we begin to see one key property of the Transformer, which is that the word in each position flows through its own path in the encoder. There are dependencies between these paths in the self-attention layer. The feed-forward layer does not have those dependencies, however, and thus the various paths can be executed in parallel while flowing through the feed-forward layer.\\nNext, we’ll switch up the example to a shorter sentence and we’ll look at what happens in each sub-layer of the encoder.\\nNow We’re Encoding!\\nAs we’ve mentioned already, an encoder receives a list of vectors as input. It processes this list by passing these vectors into a ‘self-attention’ layer, then into a feed-forward neural network, then sends out the output upwards to the next encoder.\\n\\n\\n\\n  The word at each position passes through a self-attention process. Then, they each pass through a feed-forward neural network -- the exact same network with each vector flowing through it separately.\\n\\nSelf-Attention at a High Level\\nDon’t be fooled by me throwing around the word “self-attention” like it’s a concept everyone should be familiar with. I had personally never came across the concept until reading the Attention is All You Need paper. Let us distill how it works.\\nSay the following sentence is an input sentence we want to translate:\\n”The animal didn\\'t cross the street because it was too tired”\\nWhat does “it” in this sentence refer to? Is it referring to the street or to the animal? It’s a simple question to a human, but not as simple to an algorithm.\\nWhen the model is processing the word “it”, self-attention allows it to associate “it” with “animal”.\\nAs the model processes each word (each position in the input sequence), self attention allows it to look at other positions in the input sequence for clues that can help lead to a better encoding for this word.\\nIf you’re familiar with RNNs, think of how maintaining a hidden state allows an RNN to incorporate its representation of previous words/vectors it has processed with the current one it’s processing. Self-attention is the method the Transformer uses to bake the “understanding” of other relevant words into the one we’re currently processing.\\n\\n\\n\\n  As we are encoding the word \"it\" in encoder #5 (the top encoder in the stack), part of the attention mechanism was focusing on \"The Animal\", and baked a part of its representation into the encoding of \"it\".\\n\\nBe sure to check out the Tensor2Tensor notebook where you can load a Transformer model, and examine it using this interactive visualization.\\nSelf-Attention in Detail\\nLet’s first look at how to calculate self-attention using vectors, then proceed to look at how it’s actually implemented – using matrices.\\nThe first step in calculating self-attention is to create three vectors from each of the encoder’s input vectors (in this case, the embedding of each word). So for each word, we create a Query vector, a Key vector, and a Value vector. These vectors are created by multiplying the embedding by three matrices that we trained during the training process.\\nNotice that these new vectors are smaller in dimension than the embedding vector. Their dimensionality is 64, while the embedding and encoder input/output vectors have dimensionality of 512. They don’t HAVE to be smaller, this is an architecture choice to make the computation of multiheaded attention (mostly) constant.\\n\\n\\n\\n\\n  Multiplying x1 by the WQ weight matrix produces q1, the \"query\" vector associated with that word. We end up creating a \"query\", a \"key\", and a \"value\" projection of each word in the input sentence.\\n\\n\\n\\nWhat are the “query”, “key”, and “value” vectors?\\n\\n\\nThey’re abstractions that are useful for calculating and thinking about attention. Once you proceed with reading how attention is calculated below, you’ll know pretty much all you need to know about the role each of these vectors plays.\\nThe second step in calculating self-attention is to calculate a score. Say we’re calculating the self-attention for the first word in this example, “Thinking”. We need to score each word of the input sentence against this word. The score determines how much focus to place on other parts of the input sentence as we encode a word at a certain position.\\nThe score is calculated by taking the dot product of the query vector with the key vector of the respective word we’re scoring. So if we’re processing the self-attention for the word in position #1, the first score would be the dot product of q1 and k1. The second score would be the dot product of q1 and k2.\\n\\n\\n\\n\\n\\n\\nThe third and fourth steps are to divide the scores by 8 (the square root of the dimension of the key vectors used in the paper – 64. This leads to having more stable gradients. There could be other possible values here, but this is the default), then pass the result through a softmax operation. Softmax normalizes the scores so they’re all positive and add up to 1.\\n\\n\\n\\n\\n\\nThis softmax score determines how much each word will be expressed at this position. Clearly the word at this position will have the highest softmax score, but sometimes it’s useful to attend to another word that is relevant to the current word.\\n\\nThe fifth step is to multiply each value vector by the softmax score (in preparation to sum them up). The intuition here is to keep intact the values of the word(s) we want to focus on, and drown-out irrelevant words (by multiplying them by tiny numbers like 0.001, for example).\\nThe sixth step is to sum up the weighted value vectors. This produces the output of the self-attention layer at this position (for the first word).\\n\\n\\n\\n\\n\\nThat concludes the self-attention calculation. The resulting vector is one we can send along to the feed-forward neural network. In the actual implementation, however, this calculation is done in matrix form for faster processing. So let’s look at that now that we’ve seen the intuition of the calculation on the word level.\\nMatrix Calculation of Self-Attention\\nThe first step is to calculate the Query, Key, and Value matrices. We do that by packing our embeddings into a matrix X, and multiplying it by the weight matrices we’ve trained (WQ, WK, WV).\\n\\n\\n\\n  Every row in the X matrix corresponds to a word in the input sentence. We again see the difference in size of the embedding vector (512, or 4 boxes in the figure), and the q/k/v vectors (64, or 3 boxes in the figure)\\n\\n\\nFinally, since we’re dealing with matrices, we can condense steps two through six in one formula to calculate the outputs of the self-attention layer.\\n\\n\\n\\n  The self-attention calculation in matrix form\\n\\n\\n\\nThe Beast With Many Heads\\nThe paper further refined the self-attention layer by adding a mechanism called “multi-headed” attention. This improves the performance of the attention layer in two ways:\\n\\n\\nIt expands the model’s ability to focus on different positions. Yes, in the example above, z1 contains a little bit of every other encoding, but it could be dominated by the actual word itself. If we’re translating a sentence like “The animal didn’t cross the street because it was too tired”, it would be useful to know which word “it” refers to.\\n\\n\\nIt gives the attention layer multiple “representation subspaces”. As we’ll see next, with multi-headed attention we have not only one, but multiple sets of Query/Key/Value weight matrices (the Transformer uses eight attention heads, so we end up with eight sets for each encoder/decoder). Each of these sets is randomly initialized. Then, after training, each set is used to project the input embeddings (or vectors from lower encoders/decoders) into a different representation subspace.\\n\\n\\n\\n\\n\\n   With multi-headed attention, we maintain separate Q/K/V weight matrices for each head resulting in different Q/K/V matrices. As we did before, we multiply X by the WQ/WK/WV matrices to produce Q/K/V matrices.\\n \\n\\nIf we do the same self-attention calculation we outlined above, just eight different times with different weight matrices, we end up with eight different Z matrices\\n\\n\\n\\n\\n\\nThis leaves us with a bit of a challenge. The feed-forward layer is not expecting eight matrices – it’s expecting a single matrix (a vector for each word). So we need a way to condense these eight down into a single matrix.\\nHow do we do that? We concat the matrices then multiply them by an additional weights matrix WO.\\n\\n\\n\\n\\nThat’s pretty much all there is to multi-headed self-attention. It’s quite a handful of matrices, I realize. Let me try to put them all in one visual so we can look at them in one place\\n\\n\\n\\n\\n\\n\\nNow that we have touched upon attention heads, let’s revisit our example from before to see where the different attention heads are focusing as we encode the word “it” in our example sentence:\\n\\n\\n\\n  As we encode the word \"it\", one attention head is focusing most on \"the animal\", while another is focusing on \"tired\" -- in a sense, the model\\'s representation of the word \"it\" bakes in some of the representation of both \"animal\" and \"tired\".\\n\\n\\nIf we add all the attention heads to the picture, however, things can be harder to interpret:\\n\\n\\n\\n\\nRepresenting The Order of The Sequence Using Positional Encoding\\nOne thing that’s missing from the model as we have described it so far is a way to account for the order of the words in the input sequence.\\nTo address this, the transformer adds a vector to each input embedding. These vectors follow a specific pattern that the model learns, which helps it determine the position of each word, or the distance between different words in the sequence. The intuition here is that adding these values to the embeddings provides meaningful distances between the embedding vectors once they’re projected into Q/K/V vectors and during dot-product attention.\\n\\n\\n\\n\\n  To give the model a sense of the order of the words, we add positional encoding vectors -- the values of which follow a specific pattern.\\n\\n\\nIf we assumed the embedding has a dimensionality of 4, the actual positional encodings would look like this:\\n\\n\\n\\n  A real example of positional encoding with a toy embedding size of 4\\n\\n\\nWhat might this pattern look like?\\nIn the following figure, each row corresponds to a positional encoding of a vector. So the first row would be the vector we’d add to the embedding of the first word in an input sequence. Each row contains 512 values – each with a value between 1 and -1. We’ve color-coded them so the pattern is visible.\\n\\n\\n\\n  A real example of positional encoding for 20 words (rows) with an embedding size of 512 (columns). You can see that it appears split in half down the center. That\\'s because the values of the left half are generated by one function (which uses sine), and the right half is generated by another function (which uses cosine). They\\'re then concatenated to form each of the positional encoding vectors.\\n\\nThe formula for positional encoding is described in the paper (section 3.5). You can see the code for generating positional encodings in get_timing_signal_1d(). This is not the only possible method for positional encoding. It, however, gives the advantage of being able to scale to unseen lengths of sequences (e.g. if our trained model is asked to translate a sentence longer than any of those in our training set).\\nJuly 2020 Update: \\nThe positional encoding shown above is from the Tensor2Tensor implementation of the Transformer. The method shown in the paper is slightly different in that it doesn’t directly concatenate, but interweaves the two signals. The following figure shows what that looks like. Here’s the code to generate it:\\n\\n\\n\\n\\nThe Residuals\\nOne detail in the architecture of the encoder that we need to mention before moving on, is that each sub-layer (self-attention, ffnn) in each encoder has a residual connection around it, and is followed by a layer-normalization step.\\n\\n\\n\\n\\nIf we’re to visualize the vectors and the layer-norm operation associated with self attention, it would look like this:\\n\\n\\n\\n\\nThis goes for the sub-layers of the decoder as well. If we’re to think of a Transformer of 2 stacked encoders and decoders, it would look something like this:\\n\\n\\n\\n\\nThe Decoder Side\\nNow that we’ve covered most of the concepts on the encoder side, we basically know how the components of decoders work as well. But let’s take a look at how they work together.\\nThe encoder start by processing the input sequence. The output of the top encoder is then transformed into a set of attention vectors K and V. These are to be used by each decoder in its “encoder-decoder attention” layer which helps the decoder focus on appropriate places in the input sequence:\\n\\n\\n\\n  After finishing the encoding phase, we begin the decoding phase. Each step in the decoding phase outputs an element from the output sequence (the English translation sentence in this case).\\n\\nThe following steps repeat the process until a special  symbol is reached indicating the transformer decoder has completed its output. The output of each step is fed to the bottom decoder in the next time step, and the decoders bubble up their decoding results just like the encoders did. And just like we did with the encoder inputs, we embed and add positional encoding to those decoder inputs to indicate the position of each word.\\n\\n\\n\\n\\nThe self attention layers in the decoder operate in a slightly different way than the one in the encoder:\\nIn the decoder, the self-attention layer is only allowed to attend to earlier positions in the output sequence. This is done by masking future positions (setting them to -inf) before the softmax step in the self-attention calculation.\\nThe “Encoder-Decoder Attention” layer works just like multiheaded self-attention, except it creates its Queries matrix from the layer below it, and takes the Keys and Values matrix from the output of the encoder stack.\\nThe Final Linear and Softmax Layer\\nThe decoder stack outputs a vector of floats. How do we turn that into a word? That’s the job of the final Linear layer which is followed by a Softmax Layer.\\nThe Linear layer is a simple fully connected neural network that projects the vector produced by the stack of decoders, into a much, much larger vector called a logits vector.\\nLet’s assume that our model knows 10,000 unique English words (our model’s “output vocabulary”) that it’s learned from its training dataset. This would make the logits vector 10,000 cells wide – each cell corresponding to the score of a unique word. That is how we interpret the output of the model followed by the Linear layer.\\nThe softmax layer then turns those scores into probabilities (all positive, all add up to 1.0). The cell with the highest probability is chosen, and the word associated with it is produced as the output for this time step.\\n\\n\\n\\n\\n  This figure starts from the bottom with the vector produced as the output of the decoder stack. It is then turned into an output word.\\n\\n\\nRecap Of Training\\nNow that we’ve covered the entire forward-pass process through a trained Transformer, it would be useful to glance at the intuition of training the model.\\nDuring training, an untrained model would go through the exact same forward pass. But since we are training it on a labeled training dataset, we can compare its output with the actual correct output.\\nTo visualize this, let’s assume our output vocabulary only contains six words(“a”, “am”, “i”, “thanks”, “student”, and “<eos>” (short for ‘end of sentence’)).\\n\\n\\n\\n   The output vocabulary of our model is created in the preprocessing phase before we even begin training.\\n \\nOnce we define our output vocabulary, we can use a vector of the same width to indicate each word in our vocabulary. This also known as one-hot encoding. So for example, we can indicate the word “am” using the following vector:\\n\\n\\n\\n  Example: one-hot encoding of our output vocabulary\\n\\nFollowing this recap, let’s discuss the model’s loss function – the metric we are optimizing during the training phase to lead up to a trained and hopefully amazingly accurate model.\\nThe Loss Function\\nSay we are training our model. Say it’s our first step in the training phase, and we’re training it on a simple example – translating “merci” into “thanks”.\\nWhat this means, is that we want the output to be a probability distribution indicating the word “thanks”. But since this model is not yet trained, that’s unlikely to happen just yet.\\n\\n\\n\\n  Since the model\\'s parameters (weights) are all initialized randomly, the (untrained) model produces a probability distribution with arbitrary values for each cell/word. We can compare it with the actual output, then tweak all the model\\'s weights using backpropagation to make the output closer to the desired output.\\n\\n\\nHow do you compare two probability distributions? We simply subtract one from the other. For more details, look at  cross-entropy and Kullback–Leibler divergence.\\nBut note that this is an oversimplified example. More realistically, we’ll use a sentence longer than one word. For example – input: “je suis étudiant” and expected output: “i am a student”. What this really means, is that we want our model to successively output probability distributions where:\\n\\nEach probability distribution is represented by a vector of width vocab_size (6 in our toy example, but more realistically a number like 30,000 or 50,000)\\nThe first probability distribution has the highest probability at the cell associated with the word “i”\\nThe second probability distribution has the highest probability at the cell associated with the word “am”\\nAnd so on, until the fifth output distribution indicates ‘<end of sentence>’ symbol, which also has a cell associated with it from the 10,000 element vocabulary.\\n\\n\\n\\n\\n   The targeted probability distributions we\\'ll train our model against in the training example for one sample sentence.\\n \\n\\nAfter training the model for enough time on a large enough dataset, we would hope the produced probability distributions would look like this:\\n\\n\\n\\n    Hopefully upon training, the model would output the right translation we expect. Of course it\\'s no real indication if this phrase was part of the training dataset (see: cross validation). Notice that every position gets a little bit of probability even if it\\'s unlikely to be the output of that time step -- that\\'s a very useful property of softmax which helps the training process.\\n\\nNow, because the model produces the outputs one at a time, we can assume that the model is selecting the word with the highest probability from that probability distribution and throwing away the rest. That’s one way to do it (called greedy decoding). Another way to do it would be to hold on to, say, the top two words (say, ‘I’ and ‘a’ for example), then in the next step, run the model twice: once assuming the first output position was the word ‘I’, and another time assuming the first output position was the word ‘a’, and whichever version produced less error considering both positions #1 and #2 is kept. We repeat this for positions #2 and #3…etc. This method is called “beam search”, where in our example, beam_size was two (meaning that at all times, two partial hypotheses (unfinished translations) are kept in memory), and top_beams is also two (meaning we’ll return two translations). These are both hyperparameters that you can experiment with.\\nGo Forth And Transform\\nI hope you’ve found this a useful place to start to break the ice with the major concepts of the Transformer. If you want to go deeper, I’d suggest these next steps:\\n\\nRead the Attention Is All You Need paper, the Transformer blog post (Transformer: A Novel Neural Network Architecture for Language Understanding), and the Tensor2Tensor announcement.\\nWatch Łukasz Kaiser’s talk walking through the model and its details\\nPlay with the Jupyter Notebook provided as part of the Tensor2Tensor repo\\nExplore the Tensor2Tensor repo.\\n\\nFollow-up works:\\n\\nDepthwise Separable Convolutions for Neural Machine Translation\\nOne Model To Learn Them All\\nDiscrete Autoencoders for Sequence Models\\nGenerating Wikipedia by Summarizing Long Sequences\\nImage Transformer\\nTraining Tips for the Transformer Model\\nSelf-Attention with Relative Position Representations\\nFast Decoding in Sequence Models using Discrete Latent Variables\\nAdafactor: Adaptive Learning Rates with Sublinear Memory Cost\\n\\nAcknowledgements\\nThanks to Illia Polosukhin, Jakob Uszkoreit, Llion Jones , Lukasz Kaiser, Niki Parmar, and Noam Shazeer for providing feedback on earlier versions of this post.\\nPlease hit me up on Twitter for any corrections or feedback.\\n\\n\\n    Written on June 27, 2018\\n  \\n')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content = loader.load()\n",
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import ArxivLoader\n",
    "\n",
    "# Supports all arguments of `ArxivAPIWrapper`\n",
    "docs = ArxivLoader(\n",
    "    query=\"1706.03762\",\n",
    "    load_max_docs=2\n",
    ").load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023\\n1\\nIntroduction\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [35, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2\\nBackground\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3\\nModel Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2\\nFigure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1\\nEncoder and Decoder Stacks\\nEncoder:\\nThe encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder:\\nThe decoder is also composed of a stack of N = 6 identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2\\nAttention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3\\nScaled Dot-Product Attention\\nMulti-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1\\nScaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V ) = softmax(QKT\\n√dk\\n)V\\n(1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof\\n1\\n√dk . Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by\\n1\\n√dk .\\n3.2.2\\nMulti-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4\\noutput values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\\nwhere headi = Attention(QW Q\\ni , KW K\\ni , V W V\\ni )\\nWhere the projections are parameter matrices W Q\\ni\\n∈Rdmodel×dk, W K\\ni\\n∈Rdmodel×dk, W V\\ni\\n∈Rdmodel×dv\\nand W O ∈Rhdv×dmodel.\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3\\nApplications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3\\nPosition-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0, xW1 + b1)W2 + b2\\n(2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4\\nEmbeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by √dmodel.\\n5\\nTable 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\\nLayer Type\\nComplexity per Layer\\nSequential\\nMaximum Path Length\\nOperations\\nSelf-Attention\\nO(n2 · d)\\nO(1)\\nO(1)\\nRecurrent\\nO(n · d2)\\nO(n)\\nO(n)\\nConvolutional\\nO(k · n · d2)\\nO(1)\\nO(logk(n))\\nSelf-Attention (restricted)\\nO(r · n · d)\\nO(1)\\nO(n/r)\\n3.5\\nPositional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i) = sin(pos/100002i/dmodel)\\nPE(pos,2i+1) = cos(pos/100002i/dmodel)\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4\\nWhy Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6\\nlength n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity\\nconsiderably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5\\nTraining\\nThis section describes the training regime for our models.\\n5.1\\nTraining Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2\\nHardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3\\nOptimizer\\nWe used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate = d−0.5\\nmodel · min(step_num−0.5, step_num · warmup_steps−1.5)\\n(3)\\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps = 4000.\\n5.4\\nRegularization\\nWe employ three types of regularization during training:\\n7\\nTable 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU\\nTraining Cost (FLOPs)\\nEN-DE\\nEN-FR\\nEN-DE\\nEN-FR\\nByteNet [18]\\n23.75\\nDeep-Att + PosUnk [39]\\n39.2\\n1.0 · 1020\\nGNMT + RL [38]\\n24.6\\n39.92\\n2.3 · 1019\\n1.4 · 1020\\nConvS2S [9]\\n25.16\\n40.46\\n9.6 · 1018\\n1.5 · 1020\\nMoE [32]\\n26.03\\n40.56\\n2.0 · 1019\\n1.2 · 1020\\nDeep-Att + PosUnk Ensemble [39]\\n40.4\\n8.0 · 1020\\nGNMT + RL Ensemble [38]\\n26.30\\n41.16\\n1.8 · 1020\\n1.1 · 1021\\nConvS2S Ensemble [9]\\n26.36\\n41.29\\n7.7 · 1019\\n1.2 · 1021\\nTransformer (base model)\\n27.3\\n38.1\\n3.3 · 1018\\nTransformer (big)\\n28.4\\n41.8\\n2.3 · 1019\\nResidual Dropout\\nWe apply dropout [33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\nLabel Smoothing\\nDuring training, we employed label smoothing of value ϵls = 0.1 [36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6\\nResults\\n6.1\\nMachine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU 5.\\n6.2\\nModel Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8\\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN\\ndmodel\\ndff\\nh\\ndk\\ndv\\nPdrop\\nϵls\\ntrain\\nPPL\\nBLEU\\nparams\\nsteps\\n(dev)\\n(dev)\\n×106\\nbase\\n6\\n512\\n2048\\n8\\n64\\n64\\n0.1\\n0.1\\n100K\\n4.92\\n25.8\\n65\\n(A)\\n1\\n512\\n512\\n5.29\\n24.9\\n4\\n128\\n128\\n5.00\\n25.5\\n16\\n32\\n32\\n4.91\\n25.8\\n32\\n16\\n16\\n5.01\\n25.4\\n(B)\\n16\\n5.16\\n25.1\\n58\\n32\\n5.01\\n25.4\\n60\\n(C)\\n2\\n6.11\\n23.7\\n36\\n4\\n5.19\\n25.3\\n50\\n8\\n4.88\\n25.5\\n80\\n256\\n32\\n32\\n5.75\\n24.5\\n28\\n1024\\n128\\n128\\n4.66\\n26.0\\n168\\n1024\\n5.12\\n25.4\\n53\\n4096\\n4.75\\n26.2\\n90\\n(D)\\n0.0\\n5.77\\n24.6\\n0.2\\n4.95\\n25.5\\n0.0\\n4.67\\n25.3\\n0.2\\n5.47\\n25.7\\n(E)\\npositional embedding instead of sinusoids\\n4.92\\n25.7\\nbig\\n6\\n1024\\n4096\\n16\\n0.3\\n300K\\n4.33\\n26.4\\n213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\\nresults to the base model.\\n6.3\\nEnglish Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9\\nTable 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser\\nTraining\\nWSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37]\\nWSJ only, discriminative\\n88.3\\nPetrov et al. (2006) [29]\\nWSJ only, discriminative\\n90.4\\nZhu et al. (2013) [40]\\nWSJ only, discriminative\\n90.4\\nDyer et al. (2016) [8]\\nWSJ only, discriminative\\n91.7\\nTransformer (4 layers)\\nWSJ only, discriminative\\n91.3\\nZhu et al. (2013) [40]\\nsemi-supervised\\n91.3\\nHuang & Harper (2009) [14]\\nsemi-supervised\\n91.3\\nMcClosky et al. (2006) [26]\\nsemi-supervised\\n92.1\\nVinyals & Kaiser el al. (2014) [37]\\nsemi-supervised\\n92.1\\nTransformer (4 layers)\\nsemi-supervised\\n92.7\\nLuong et al. (2015) [23]\\nmulti-task\\n93.0\\nDyer et al. (2016) [8]\\ngenerative\\n93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7\\nConclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements\\nWe are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n10\\n[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL, 2016.\\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[10] Alex Graves.\\nGenerating sequences with recurrent neural networks.\\narXiv preprint\\narXiv:1308.0850, 2013.\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770–778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735–1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 832–841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n11\\n[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\npages 152–159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July\\n2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems, 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers), pages 434–443. ACL, August 2013.\\n12\\nAttention Visualizations\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15\\n')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONVERT THE DOCUMENT INTO DIFFERENT CHUNKS :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Attention is all you need.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Attention is all you need.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2'}, page_content='1 Introduction\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 35, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Attention is all you need.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3'}, page_content='Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Attention is all you need.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4'}, page_content='Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V) = softmax(QKT\\n√dk\\n)V (1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof 1√dk\\n. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by 1√dk\\n.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Attention is all you need.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5'}, page_content='output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V) = Concat(head1, ...,headh)WO\\nwhere headi = Attention(QWQ\\ni , KWK\\ni , V WV\\ni )\\nWhere the projections are parameter matricesWQ\\ni ∈ Rdmodel×dk , WK\\ni ∈ Rdmodel×dk , WV\\ni ∈ Rdmodel×dv\\nand WO ∈ Rhdv×dmodel .\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0, xW1 + b1)W2 + b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by √dmodel.\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Attention is all you need.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6'}, page_content='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2 · d) O(1) O(1)\\nRecurrent O(n · d2) O(n) O(n)\\nConvolutional O(k · n · d2) O(1) O(logk(n))\\nSelf-Attention (restricted) O(r · n · d) O(1) O(n/r)\\n3.5 Positional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nP E(pos,2i) = sin(pos/100002i/dmodel )\\nP E(pos,2i+1) = cos(pos/100002i/dmodel )\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k, P Epos+k can be represented as a linear function of\\nP Epos.\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈ Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Attention is all you need.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7'}, page_content='length n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < ndoes not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [ 18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\nconsiderably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2 Hardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3 Optimizer\\nWe used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate = d−0.5\\nmodel · min(step_num−0.5, step_num · warmup_steps−1.5) (3)\\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps = 4000.\\n5.4 Regularization\\nWe employ three types of regularization during training:\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Attention is all you need.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8'}, page_content='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU Training Cost (FLOPs)\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet [18] 23.75\\nDeep-Att + PosUnk [39] 39.2 1.0 · 1020\\nGNMT + RL [38] 24.6 39.92 2.3 · 1019 1.4 · 1020\\nConvS2S [9] 25.16 40.46 9.6 · 1018 1.5 · 1020\\nMoE [32] 26.03 40.56 2.0 · 1019 1.2 · 1020\\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0 · 1020\\nGNMT + RL Ensemble [38] 26.30 41.16 1.8 · 1020 1.1 · 1021\\nConvS2S Ensemble [9] 26.36 41.29 7.7 · 1019 1.2 · 1021\\nTransformer (base model) 27.3 38.1 3.3 · 1018\\nTransformer (big) 28.4 41.8 2.3 · 1019\\nResidual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\nLabel Smoothing During training, we employed label smoothing of value ϵls = 0.1 [36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6 Results\\n6.1 Machine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU 5.\\n6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Attention is all you need.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9'}, page_content='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d model dff h d k dv Pdrop ϵls\\ntrain PPL BLEU params\\nsteps (dev) (dev) ×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)\\n1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B) 16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)\\n2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)\\n0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\\nresults to the base model.\\n6.3 English Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Attention is all you need.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10'}, page_content='Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser Training WSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\\nTransformer (4 layers) WSJ only, discriminative 91.3\\nZhu et al. (2013) [40] semi-supervised 91.3\\nHuang & Harper (2009) [14] semi-supervised 91.3\\nMcClosky et al. (2006) [26] semi-supervised 92.1\\nVinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\\nTransformer (4 layers) semi-supervised 92.7\\nLuong et al. (2015) [23] multi-task 93.0\\nDyer et al. (2016) [8] generative 93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7 Conclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Attention is all you need.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11'}, page_content='[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL, 2016.\\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850, 2013.\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770–778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735–1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 832–841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time.arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Attention is all you need.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12'}, page_content='[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\npages 152–159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July\\n2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems, 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers), pages 434–443. ACL, August 2013.\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Attention is all you need.pdf', 'total_pages': 15, 'page': 12, 'page_label': '13'}, page_content='Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Attention is all you need.pdf', 'total_pages': 15, 'page': 13, 'page_label': '14'}, page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Attention is all you need.pdf', 'total_pages': 15, 'page': 14, 'page_label': '15'}, page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"Attention is all you need.pdf\")\n",
    "documents = loader.load()\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
